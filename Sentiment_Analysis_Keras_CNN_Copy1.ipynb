{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis CNN in Keras - example of suing IMDb dataset with Glove pretrained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division, print_function \n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Activation, Dropout, Flatten, Convolution1D, SpatialDropout1D, MaxPooling1D\n",
    "from keras import metrics #Only one metric is supported at the moment and that is accuracy\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'C:/Users/Gavrilov/My Projects/Sentiment Analysis CNN/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb \n",
    "    #dataset of 25,000 movies reviews from IMDB #https://keras.io/datasets/\n",
    "    #each review labeled by sentiment (positive/negative)\n",
    "    #each review is encoded as a sequence of word indexes (integers)\n",
    "    #words are indexed by overall frequency in the dataset (integer \"3\" encodes the 3rd most frequent word)\n",
    "    #\"0\" does not stand for a specific word, but instead is used to encode any unknown word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Keras words frequency - dictionary manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dictionary of word (key) and index (value) - which is necessary when passing indices to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "dataset_words_idx = imdb.get_word_index() #IMDb indices assignment to idx variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of word 'the' is 1\n",
      "{'fawn': 34701, 'tsukino': 52006, 'nunnery': 52007, 'sonja': 16816, 'vani': 63951, 'woods': 1408, 'spiders': 16115, 'hanging': 2345, 'woody': 2289, 'trawling': 52008, \"hold's\": 52009, 'comically': 11307, 'localized': 40830, 'disobeying': 30568, \"'royale\": 52010, \"harpo's\": 40831, 'canet': 52011, 'aileen': 19313, 'acurately': 52012, \"diplomat's\": 52013}\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of word 'the' is\", dataset_words_idx['the'])\n",
    "print({i:dataset_words_idx[i] for i in list(dataset_words_idx)[:20]}) #print out some elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dictionary of index (value) and word (key) - which is necessary when decoding models predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_idx_words = {value: key for key, value in iter(dataset_words_idx.items())} \n",
    "    #mapping of words to indices will give dictionary, where key and value positions switched\n",
    "#dataset_idx_words = {}\n",
    "#for key, value in iter(dataset_words_idx.items()): #iter(ind_words.items()) -iterates over the dictionary returning key, value \n",
    "#    dataset_idx_words[value]=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fawn tsukino nunnery\n",
      "{34701: 'fawn', 52006: 'tsukino', 52007: 'nunnery', 16816: 'sonja', 63951: 'vani', 1408: 'woods', 16115: 'spiders', 2345: 'hanging', 2289: 'woody', 52008: 'trawling', 52009: \"hold's\", 11307: 'comically', 40830: 'localized', 30568: 'disobeying', 52010: \"'royale\", 40831: \"harpo's\", 52011: 'canet', 19313: 'aileen', 52012: 'acurately', 52013: \"diplomat's\"}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_idx_words[34701], dataset_idx_words[52006], dataset_idx_words[52007])\n",
    "print({i:dataset_idx_words[i] for i in list(dataset_idx_words)[:20]}) #printing out of mapped elements of new dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset frequent words in sorted array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fawn tsukino nunnery\n",
      "Most frequent words are:  ['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i', 'this', 'that', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'on']\n"
     ]
    }
   ],
   "source": [
    "dataset_words_arr_sorted = sorted(dataset_words_idx, key=dataset_words_idx.get)\n",
    "    #sorted() returns sorted list but not mutates the original list\n",
    "    #idx is iterable (words in our example), key function will sort the given iterable\n",
    "    #idx.get returns value None because value in idx.get() not specified\n",
    "print(dataset_words_arr_sorted[34700], dataset_words_arr_sorted[52005], dataset_words_arr_sorted[52006])\n",
    "    #words from dataset dictionary by specific index\n",
    "print('Most frequent words are: ', dataset_words_arr_sorted[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using IMDB Keras dataset directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size=5000 #truncate vocabulary down to 5000\n",
    "review_len=500 #truncate every review to 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 12s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(reviews_train, labels_train), (reviews_test, labels_test) = imdb.load_data(path='imdb.npz',\n",
    "                                                      num_words=vocab_size, #top most frequent words to consider \n",
    "                                                      #skip_top=0, #top most frequent words to ignore\n",
    "                                                      #maxlen=review_len, #any longer sequence will be truncated\n",
    "                                                                          #will not use it here because it will cause disproportion in shapes of reviews_train and reviews_test\n",
    "                                                                          #will truncate manually instead using Keras sequence padding\n",
    "                                                      #seed=500, \n",
    "                                                      start_char=1, #the start of a sequence will be marked with this character\n",
    "                                                                     #set to 1 because 0 is usually the padding character        \n",
    "                                                      #ovv_char=0, #words that were cut out because of the num_words or skip_top limit will be replaced with this character  \n",
    "                                                      index_from=0) #index actual words with this index and higher\n",
    "    #reviews_train, reviews_test -list of sequences, which are lists of indexes (integers) (list or reviews which captures list of indices of words)\n",
    "    #labels_train, labels_test -list of integer labels (1 positive or 0 negative sentiment) given to reviews in dataset\n",
    "    #labeled reviews it's pretty much same as labeled pictures in Dogs vs Cats task\n",
    "    #what the convolutional neural net does is learning to predict where is negarive or positive review on unlabeled data (Dog vs Cat in the example with pictures) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_train), len(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 11,\n",
       " 19,\n",
       " 13,\n",
       " 40,\n",
       " 527,\n",
       " 970,\n",
       " 1619,\n",
       " 1382,\n",
       " 62,\n",
       " 455,\n",
       " 4465,\n",
       " 63,\n",
       " 3938,\n",
       " 1,\n",
       " 170,\n",
       " 33,\n",
       " 253,\n",
       " 2,\n",
       " 22,\n",
       " 97,\n",
       " 40,\n",
       " 835,\n",
       " 109,\n",
       " 47,\n",
       " 667,\n",
       " 2,\n",
       " 6,\n",
       " 32,\n",
       " 477,\n",
       " 281,\n",
       " 2,\n",
       " 147,\n",
       " 1,\n",
       " 169,\n",
       " 109,\n",
       " 164,\n",
       " 2,\n",
       " 333,\n",
       " 382,\n",
       " 36,\n",
       " 1,\n",
       " 169,\n",
       " 4533,\n",
       " 1108,\n",
       " 14,\n",
       " 543,\n",
       " 35,\n",
       " 10,\n",
       " 444,\n",
       " 1,\n",
       " 189,\n",
       " 47,\n",
       " 13,\n",
       " 3,\n",
       " 144,\n",
       " 2022,\n",
       " 16,\n",
       " 11,\n",
       " 19,\n",
       " 1,\n",
       " 1917,\n",
       " 4610,\n",
       " 466,\n",
       " 1,\n",
       " 19,\n",
       " 68,\n",
       " 84,\n",
       " 9,\n",
       " 13,\n",
       " 40,\n",
       " 527,\n",
       " 35,\n",
       " 73,\n",
       " 12,\n",
       " 10,\n",
       " 1244,\n",
       " 1,\n",
       " 19,\n",
       " 14,\n",
       " 512,\n",
       " 14,\n",
       " 9,\n",
       " 13,\n",
       " 623,\n",
       " 15,\n",
       " 2,\n",
       " 2,\n",
       " 59,\n",
       " 383,\n",
       " 9,\n",
       " 5,\n",
       " 313,\n",
       " 5,\n",
       " 103,\n",
       " 2,\n",
       " 1,\n",
       " 2220,\n",
       " 2,\n",
       " 13,\n",
       " 477,\n",
       " 63,\n",
       " 3782,\n",
       " 30,\n",
       " 1,\n",
       " 127,\n",
       " 9,\n",
       " 13,\n",
       " 35,\n",
       " 616,\n",
       " 2,\n",
       " 22,\n",
       " 121,\n",
       " 48,\n",
       " 33,\n",
       " 132,\n",
       " 45,\n",
       " 22,\n",
       " 1412,\n",
       " 30,\n",
       " 3,\n",
       " 19,\n",
       " 9,\n",
       " 212,\n",
       " 25,\n",
       " 74,\n",
       " 49,\n",
       " 2,\n",
       " 11,\n",
       " 404,\n",
       " 13,\n",
       " 79,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 104,\n",
       " 114,\n",
       " 2,\n",
       " 12,\n",
       " 253,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3763,\n",
       " 2,\n",
       " 720,\n",
       " 33,\n",
       " 68,\n",
       " 40,\n",
       " 527,\n",
       " 473,\n",
       " 23,\n",
       " 397,\n",
       " 314,\n",
       " 43,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1026,\n",
       " 10,\n",
       " 101,\n",
       " 85,\n",
       " 1,\n",
       " 378,\n",
       " 12,\n",
       " 294,\n",
       " 95,\n",
       " 29,\n",
       " 2068,\n",
       " 53,\n",
       " 23,\n",
       " 138,\n",
       " 3,\n",
       " 191,\n",
       " 2,\n",
       " 15,\n",
       " 1,\n",
       " 223,\n",
       " 19,\n",
       " 18,\n",
       " 131,\n",
       " 473,\n",
       " 23,\n",
       " 477,\n",
       " 2,\n",
       " 141,\n",
       " 27,\n",
       " 2,\n",
       " 15,\n",
       " 48,\n",
       " 33,\n",
       " 25,\n",
       " 221,\n",
       " 89,\n",
       " 22,\n",
       " 101,\n",
       " 1,\n",
       " 223,\n",
       " 62,\n",
       " 13,\n",
       " 35,\n",
       " 1331,\n",
       " 85,\n",
       " 9,\n",
       " 13,\n",
       " 280,\n",
       " 2,\n",
       " 13,\n",
       " 4469,\n",
       " 110,\n",
       " 100,\n",
       " 29,\n",
       " 12,\n",
       " 13,\n",
       " 2,\n",
       " 16,\n",
       " 175,\n",
       " 29]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert and is an amazing actor and now the same being director and father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for and and would recommend it to everyone to watch and the fly and was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also and to the two little and that played the and of norman and paul they were just brilliant children are often left out of the and list i think because the stars that play them all grown up are such a big and for the whole film but these children are amazing and should be and for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was and with us all\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([dataset_idx_words[i] for i in reviews_train[0]]) \n",
    "#so our goal is to take 25,000 reviews, and predict whether it will be positive or negative in sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0], dtype=int64),\n",
       " array([0, 1, 1, 0, 1, 1, 1, 0, 0, 1], dtype=int64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[:10], labels_test[:10]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rectangular matrix - Zero padding of each sequence for making consistent length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train.shape, labels_test.shape #given shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141, 603)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_train[2]), len(reviews_test[2]) #we need to make all of reviews the same lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_train_padded = pad_sequences(reviews_train, maxlen=500, value=0) \n",
    "reviews_test_padded = pad_sequences(reviews_test, maxlen=500, value=0)\n",
    "#truncates everything greater than maxlen, but padds everything with given value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 500), (25000, 500))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train_padded.shape, reviews_test_padded.shape #at the end of this we have numpy array with identical shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    1,   11,   44,    5,\n",
       "          27,   28,    4,    1,  246,  105,    4,    1,    2,   51,   58,\n",
       "         366,   10,   68,  146,   11,   19,  109,    1, 2398,  308,    9,\n",
       "          13, 3708,   30,   72,   40, 1826,  293,    1,   83,  317,   32,\n",
       "         531,   16,  260, 4818, 1298,    1, 1870,   30,   86,   75,    9,\n",
       "          63,   13,    1,  357,    4,    1,   55,  313,  331,    8,    1,\n",
       "        1713,   40,  642,  659,    5,  254,   82, 1197,   39, 1225, 2575,\n",
       "          80,   65, 3909,   12,   33,  162, 1536,  275,   33,   66,    2,\n",
       "         777,    5,  103,   11,    2, 1335,   15,    3,   19,    9,  212,\n",
       "          25,  607,   37,    3,   84,  323,   20, 2297,   18,   20,   19,\n",
       "           9,  269,   37,   54,   28,    8,    1,   19,   44,    3, 2304,\n",
       "          48,    6,  167,   20,  592,  113,  592, 1349,   10,  188,   76,\n",
       "         635,   86,    2,   11,    6,    5,  103,  604,  621,   32,  531,\n",
       "           3,  224,    4,  126,  110]),\n",
       " array([  30,    3,   55,   51, 1267,  428,  745,    4,   29, 2577,   13,\n",
       "           8,   91,    2,    7,    7,    1,  990,    2,    4,    1, 1763,\n",
       "        2631, 2161,    2,    5,  844,    5, 1447,  118,   28,    4,   24,\n",
       "          83, 2660,    2,   13,    3,  462,  990, 2003,    2,  570,   14,\n",
       "           2,   39,    1,    2,   34,  470,    3,  708,    3,    2,    4,\n",
       "         325,  209,   67,   27,  255,    8,  217,   29,    4,  105,   18,\n",
       "         130,    9,    6,   52,  462,  846, 3708,   50,   30, 2068, 1966,\n",
       "          34,   67, 1141,    1,    2, 1406,   71,  473,   34,   59,   88,\n",
       "        1326,  166,    1, 1327,    2,  143,  652, 2209,    2,  255,    9,\n",
       "         181,    2,  543,    2,  846,    2,    4,    1,   19, 1433,   15,\n",
       "         628, 1383,  794,    4,    1,    2,   68,  345,  422, 4317, 1058,\n",
       "          16,    2,    2,    2,    8,  658,    5,  336,    2,    1, 2452,\n",
       "           2,    4,    1, 1959,    7,    7,  260,  784,    6,  267,    8,\n",
       "           3,    2,    1,    2,    2,  118,    1,    2,   23, 4431,   16,\n",
       "          65, 1369,    2,   25,  443,    3,  315,    2,    5,   64,   48,\n",
       "          33,   67,   78,    5, 4389, 2291,   33, 1194,    5,    2,    2,\n",
       "          15,    3,  708,    1,    2,   23,    2, 1122,    8,   11,  633,\n",
       "         717,    9,  423,   25,   74,  773,    5,   94,   35,  108,    2,\n",
       "           2,  165, 1236,    2,  134,    2,   15,   24,  170,    6, 2396,\n",
       "          14,    3,    2,  425,    2,  229,    8,    1,    2,   34,  269,\n",
       "          37, 2705,  244,   27,  653,    3,    2,   51,    2, 3289,   95,\n",
       "           3, 2837,   37,  555,   34,    2,   95,    1,    2, 1194,   12,\n",
       "          11,    6,   54, 4890,    2, 4656,    3,  272,  708,    2,    2,\n",
       "        3289,   95,    3,    2,    7,    7,    2,   16,   11,    2,  264,\n",
       "         159,  708,   34,    2,  749,   95,    1,    2, 2375,   87,   16,\n",
       "           3,    2,    4,    2, 1807,    2,    1, 4767, 3180,  927,    5,\n",
       "         505,   87,    1, 1314,    5,    1,    2,   14,    2, 3962, 1850,\n",
       "           1, 1491,    5, 4465,  186,    1,    2,    2,    2,    1, 4767,\n",
       "           2,   92,  268,   20,    3,    2,    2,    2,    2,   30, 1523,\n",
       "           3,  422, 3152,    2, 4532, 1633,    4,    1, 4666,    2,  466,\n",
       "           1, 4549,   51,    1,  147,    2,    2,  277,   50,    2,    2,\n",
       "          15,  336,   26, 1975,   24,    2,    2,    2,   65, 1827,   16,\n",
       "           2,    2,    1, 1512,    4,  260,   62, 2129,   31,    3,    2,\n",
       "           2,   40,  156,   26,    6, 4703,    6,  384,   70,  192,  581,\n",
       "           7,    7, 1066,    1,   55,  807,   51,   11,    2,  114,   19,\n",
       "          13,   90,    2, 1066,    1,  189,   12,    9,   13,   90,   31,\n",
       "           3, 1763,    2,   30,    1,    2,    4,   12,    2,    2, 3283,\n",
       "         322,    9,   59,   27,  773,    5,   64,   11,   14,    3,    2,\n",
       "          41,  145,  684,    2,  200,   39,  200,   21,   25,   66,    2,\n",
       "           2,    8,  327,   51,   26,   90,    2,   18,  842,    2,   24,\n",
       "        1096,    4,  816,    1,   19, 1404,   14,    3,    2,  784,    4,\n",
       "        2457,    2,    2,   97,   27,    1, 3734, 3614, 3166, 2318,   39,\n",
       "        1895,    8,    1, 3811,   39,   98,  701,    4,   98,  996,   12,\n",
       "        1622,   91, 2923,  177,    2,    6,    2,   31,    2,   42,    3,\n",
       "        1426,   19,   57,    3, 1217,   28,    8,   91,    2,   93,   18,\n",
       "          91,  746,    6,   54,  972]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train_padded[2], reviews_test_padded[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train_padded.shape==reviews_test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_train_padded[2])==len(reviews_test_padded[2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Pretrained Word Embeddings setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://nlp.stanford.edu/data/glove.6B.zip\n",
      "862183424/862182613 [==============================] - 448s 1us/step\n"
     ]
    }
   ],
   "source": [
    "glove_6B = get_file('glove.6B.zip', origin='http://nlp.stanford.edu/data/glove.6B.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unzipping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(glove_6B, 'r') as zip_ref:\n",
    "    zip_ref.extractall('C:/Users/Gavrilov/.keras/datasets/glove_6B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### setting up a variable of 6B version tokens Glove with 50 dimensions words embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = \"glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reading file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('C:/Users/Gavrilov/.keras/datasets/glove_6B/'+name, 'r', encoding=\"utf8\") as f: \n",
    "    lines = [line.split() for line in f] #lines.split() -without value '\\n' because each iteration gives line and that is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " '0.418',\n",
       " '0.24968',\n",
       " '-0.41242',\n",
       " '0.1217',\n",
       " '0.34527',\n",
       " '-0.044457',\n",
       " '-0.49688',\n",
       " '-0.17862',\n",
       " '-0.00066023',\n",
       " '-0.6566',\n",
       " '0.27843',\n",
       " '-0.14767',\n",
       " '-0.55677',\n",
       " '0.14658',\n",
       " '-0.0095095',\n",
       " '0.011658',\n",
       " '0.10204',\n",
       " '-0.12792',\n",
       " '-0.8443',\n",
       " '-0.12181',\n",
       " '-0.016801',\n",
       " '-0.33279',\n",
       " '-0.1552',\n",
       " '-0.23131',\n",
       " '-0.19181',\n",
       " '-1.8823',\n",
       " '-0.76746',\n",
       " '0.099051',\n",
       " '-0.42125',\n",
       " '-0.19526',\n",
       " '4.0071',\n",
       " '-0.18594',\n",
       " '-0.52287',\n",
       " '-0.31681',\n",
       " '0.00059213',\n",
       " '0.0074449',\n",
       " '0.17778',\n",
       " '-0.15897',\n",
       " '0.012041',\n",
       " '-0.054223',\n",
       " '-0.29871',\n",
       " '-0.15749',\n",
       " '-0.34758',\n",
       " '-0.045637',\n",
       " '-0.44251',\n",
       " '0.18785',\n",
       " '0.0027849',\n",
       " '-0.18411',\n",
       " '-0.11514',\n",
       " '-0.78581']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### setting up main arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_words = [elem[0] for elem in lines]\n",
    "glove_words_idx = {elem:idx for idx,elem in enumerate(glove_words)} #is elem:idx equal to glove_words_idx[elem]=idx?\n",
    "glove_vecs = np.stack(np.array(elem[1:], dtype=np.float32) for elem in lines) #np.float32 -standard double-precision floating point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the , . of to and in a \" \\'s for - that on is was said with he as',\n",
       " 0,\n",
       " (400000, 50),\n",
       " array([  4.18000013e-01,   2.49679998e-01,  -4.12420005e-01,\n",
       "          1.21699996e-01,   3.45270008e-01,  -4.44569997e-02,\n",
       "         -4.96879995e-01,  -1.78619996e-01,  -6.60229998e-04,\n",
       "         -6.56599998e-01,   2.78430015e-01,  -1.47670001e-01,\n",
       "         -5.56770027e-01,   1.46579996e-01,  -9.50950012e-03,\n",
       "          1.16579998e-02,   1.02040000e-01,  -1.27920002e-01,\n",
       "         -8.44299972e-01,  -1.21809997e-01,  -1.68009996e-02,\n",
       "         -3.32789987e-01,  -1.55200005e-01,  -2.31309995e-01,\n",
       "         -1.91809997e-01,  -1.88230002e+00,  -7.67459989e-01,\n",
       "          9.90509987e-02,  -4.21249986e-01,  -1.95260003e-01,\n",
       "          4.00710011e+00,  -1.85939997e-01,  -5.22870004e-01,\n",
       "         -3.16810012e-01,   5.92130003e-04,   7.44489999e-03,\n",
       "          1.77780002e-01,  -1.58969998e-01,   1.20409997e-02,\n",
       "         -5.42230010e-02,  -2.98709989e-01,  -1.57490000e-01,\n",
       "         -3.47579986e-01,  -4.56370004e-02,  -4.42510009e-01,\n",
       "          1.87849998e-01,   2.78489990e-03,  -1.84110001e-01,\n",
       "         -1.15139998e-01,  -7.85809994e-01], dtype=float32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(glove_words[:20]), glove_words_idx['the'], glove_vecs.shape, glove_vecs[glove_words_idx['the']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(glove_words, open('C:/Users/Gavrilov/.keras/datasets/glove_6B/'+name+'_glove_words.pkl', 'wb'))\n",
    "pickle.dump(glove_words_idx, open('C:/Users/Gavrilov/.keras/datasets/glove_6B/'+name+'_glove_words_idx.pkl', 'wb'))\n",
    "np.save('C:/Users/Gavrilov/.keras/datasets/glove_6B/'+name+'_glove_vecs', glove_vecs, allow_pickle=True) #file would have a name glove.6B.50d.txt.dat which is a little buggy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loading results (if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " ',',\n",
       " '.',\n",
       " 'of',\n",
       " 'to',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " '\"',\n",
       " \"'s\",\n",
       " 'for',\n",
       " '-',\n",
       " 'that',\n",
       " 'on',\n",
       " 'is',\n",
       " 'was',\n",
       " 'said',\n",
       " 'with',\n",
       " 'he',\n",
       " 'as',\n",
       " 'it',\n",
       " 'by',\n",
       " 'at',\n",
       " '(',\n",
       " ')',\n",
       " 'from',\n",
       " 'his',\n",
       " \"''\",\n",
       " '``',\n",
       " 'an',\n",
       " 'be',\n",
       " 'has',\n",
       " 'are',\n",
       " 'have',\n",
       " 'but',\n",
       " 'were',\n",
       " 'not',\n",
       " 'this',\n",
       " 'who',\n",
       " 'they',\n",
       " 'had',\n",
       " 'i',\n",
       " 'which',\n",
       " 'will',\n",
       " 'their',\n",
       " ':',\n",
       " 'or',\n",
       " 'its',\n",
       " 'one',\n",
       " 'after',\n",
       " 'new',\n",
       " 'been',\n",
       " 'also',\n",
       " 'we',\n",
       " 'would',\n",
       " 'two',\n",
       " 'more',\n",
       " \"'\",\n",
       " 'first',\n",
       " 'about',\n",
       " 'up',\n",
       " 'when',\n",
       " 'year',\n",
       " 'there',\n",
       " 'all',\n",
       " '--',\n",
       " 'out',\n",
       " 'she',\n",
       " 'other',\n",
       " 'people',\n",
       " \"n't\",\n",
       " 'her',\n",
       " 'percent',\n",
       " 'than',\n",
       " 'over',\n",
       " 'into',\n",
       " 'last',\n",
       " 'some',\n",
       " 'government',\n",
       " 'time',\n",
       " '$',\n",
       " 'you',\n",
       " 'years',\n",
       " 'if',\n",
       " 'no',\n",
       " 'world',\n",
       " 'can',\n",
       " 'three',\n",
       " 'do',\n",
       " ';',\n",
       " 'president',\n",
       " 'only',\n",
       " 'state',\n",
       " 'million',\n",
       " 'could',\n",
       " 'us',\n",
       " 'most',\n",
       " '_',\n",
       " 'against',\n",
       " 'u.s.',\n",
       " 'so',\n",
       " 'them',\n",
       " 'what',\n",
       " 'him',\n",
       " 'united',\n",
       " 'during',\n",
       " 'before',\n",
       " 'may',\n",
       " 'since',\n",
       " 'many',\n",
       " 'while',\n",
       " 'where',\n",
       " 'states',\n",
       " 'because',\n",
       " 'now',\n",
       " 'city',\n",
       " 'made',\n",
       " 'like',\n",
       " 'between',\n",
       " 'did',\n",
       " 'just',\n",
       " 'national',\n",
       " 'day',\n",
       " 'country',\n",
       " 'under',\n",
       " 'such',\n",
       " 'second',\n",
       " 'then',\n",
       " 'company',\n",
       " 'group',\n",
       " 'any',\n",
       " 'through',\n",
       " 'china',\n",
       " 'four',\n",
       " 'being',\n",
       " 'down',\n",
       " 'war',\n",
       " 'back',\n",
       " 'off',\n",
       " 'south',\n",
       " 'american',\n",
       " 'minister',\n",
       " 'police',\n",
       " 'well',\n",
       " 'including',\n",
       " 'team',\n",
       " 'international',\n",
       " 'week',\n",
       " 'officials',\n",
       " 'still',\n",
       " 'both',\n",
       " 'even',\n",
       " 'high',\n",
       " 'part',\n",
       " 'told',\n",
       " 'those',\n",
       " 'end',\n",
       " 'former',\n",
       " 'these',\n",
       " 'make',\n",
       " 'billion',\n",
       " 'work',\n",
       " 'our',\n",
       " 'home',\n",
       " 'school',\n",
       " 'party',\n",
       " 'house',\n",
       " 'old',\n",
       " 'later',\n",
       " 'get',\n",
       " 'another',\n",
       " 'tuesday',\n",
       " 'news',\n",
       " 'long',\n",
       " 'five',\n",
       " 'called',\n",
       " '1',\n",
       " 'wednesday',\n",
       " 'military',\n",
       " 'way',\n",
       " 'used',\n",
       " 'much',\n",
       " 'next',\n",
       " 'monday',\n",
       " 'thursday',\n",
       " 'friday',\n",
       " 'game',\n",
       " 'here',\n",
       " '?',\n",
       " 'should',\n",
       " 'take',\n",
       " 'very',\n",
       " 'my',\n",
       " 'north',\n",
       " 'security',\n",
       " 'season',\n",
       " 'york',\n",
       " 'how',\n",
       " 'public',\n",
       " 'early',\n",
       " 'according',\n",
       " 'several',\n",
       " 'court',\n",
       " 'say',\n",
       " 'around',\n",
       " 'foreign',\n",
       " '10',\n",
       " 'until',\n",
       " 'set',\n",
       " 'political',\n",
       " 'says',\n",
       " 'market',\n",
       " 'however',\n",
       " 'family',\n",
       " 'life',\n",
       " 'same',\n",
       " 'general',\n",
       " '–',\n",
       " 'left',\n",
       " 'good',\n",
       " 'top',\n",
       " 'university',\n",
       " 'going',\n",
       " 'number',\n",
       " 'major',\n",
       " 'known',\n",
       " 'points',\n",
       " 'won',\n",
       " 'six',\n",
       " 'month',\n",
       " 'dollars',\n",
       " 'bank',\n",
       " '2',\n",
       " 'iraq',\n",
       " 'use',\n",
       " 'members',\n",
       " 'each',\n",
       " 'area',\n",
       " 'found',\n",
       " 'official',\n",
       " 'sunday',\n",
       " 'place',\n",
       " 'go',\n",
       " 'based',\n",
       " 'among',\n",
       " 'third',\n",
       " 'times',\n",
       " 'took',\n",
       " 'right',\n",
       " 'days',\n",
       " 'local',\n",
       " 'economic',\n",
       " 'countries',\n",
       " 'see',\n",
       " 'best',\n",
       " 'report',\n",
       " 'killed',\n",
       " 'held',\n",
       " 'business',\n",
       " 'west',\n",
       " 'does',\n",
       " 'own',\n",
       " '%',\n",
       " 'came',\n",
       " 'law',\n",
       " 'months',\n",
       " 'women',\n",
       " \"'re\",\n",
       " 'power',\n",
       " 'think',\n",
       " 'service',\n",
       " 'children',\n",
       " 'bush',\n",
       " 'show',\n",
       " '/',\n",
       " 'help',\n",
       " 'chief',\n",
       " 'saturday',\n",
       " 'system',\n",
       " 'john',\n",
       " 'support',\n",
       " 'series',\n",
       " 'play',\n",
       " 'office',\n",
       " 'following',\n",
       " 'me',\n",
       " 'meeting',\n",
       " 'expected',\n",
       " 'late',\n",
       " 'washington',\n",
       " 'games',\n",
       " 'european',\n",
       " 'league',\n",
       " 'reported',\n",
       " 'final',\n",
       " 'added',\n",
       " 'without',\n",
       " 'british',\n",
       " 'white',\n",
       " 'history',\n",
       " 'man',\n",
       " 'men',\n",
       " 'became',\n",
       " 'want',\n",
       " 'march',\n",
       " 'case',\n",
       " 'few',\n",
       " 'run',\n",
       " 'money',\n",
       " 'began',\n",
       " 'open',\n",
       " 'name',\n",
       " 'trade',\n",
       " 'center',\n",
       " '3',\n",
       " 'israel',\n",
       " 'oil',\n",
       " 'too',\n",
       " 'al',\n",
       " 'film',\n",
       " 'win',\n",
       " 'led',\n",
       " 'east',\n",
       " 'central',\n",
       " '20',\n",
       " 'air',\n",
       " 'come',\n",
       " 'chinese',\n",
       " 'town',\n",
       " 'leader',\n",
       " 'army',\n",
       " 'line',\n",
       " 'never',\n",
       " 'little',\n",
       " 'played',\n",
       " 'prime',\n",
       " 'death',\n",
       " 'companies',\n",
       " 'least',\n",
       " 'put',\n",
       " 'forces',\n",
       " 'past',\n",
       " 'de',\n",
       " 'half',\n",
       " 'june',\n",
       " 'saying',\n",
       " 'know',\n",
       " 'federal',\n",
       " 'french',\n",
       " 'peace',\n",
       " 'earlier',\n",
       " 'capital',\n",
       " 'force',\n",
       " 'great',\n",
       " 'union',\n",
       " 'near',\n",
       " 'released',\n",
       " 'small',\n",
       " 'department',\n",
       " 'every',\n",
       " 'health',\n",
       " 'japan',\n",
       " 'head',\n",
       " 'ago',\n",
       " 'night',\n",
       " 'big',\n",
       " 'cup',\n",
       " 'election',\n",
       " 'region',\n",
       " 'director',\n",
       " 'talks',\n",
       " 'program',\n",
       " 'far',\n",
       " 'today',\n",
       " 'statement',\n",
       " 'july',\n",
       " 'although',\n",
       " 'district',\n",
       " 'again',\n",
       " 'born',\n",
       " 'development',\n",
       " 'leaders',\n",
       " 'council',\n",
       " 'close',\n",
       " 'record',\n",
       " 'along',\n",
       " 'county',\n",
       " 'france',\n",
       " 'went',\n",
       " 'point',\n",
       " 'must',\n",
       " 'spokesman',\n",
       " 'your',\n",
       " 'member',\n",
       " 'plan',\n",
       " 'financial',\n",
       " 'april',\n",
       " 'recent',\n",
       " 'campaign',\n",
       " 'become',\n",
       " 'troops',\n",
       " 'whether',\n",
       " 'lost',\n",
       " 'music',\n",
       " '15',\n",
       " 'got',\n",
       " 'israeli',\n",
       " '30',\n",
       " 'need',\n",
       " '4',\n",
       " 'lead',\n",
       " 'already',\n",
       " 'russia',\n",
       " 'though',\n",
       " 'might',\n",
       " 'free',\n",
       " 'hit',\n",
       " 'rights',\n",
       " '11',\n",
       " 'information',\n",
       " 'away',\n",
       " '12',\n",
       " '5',\n",
       " 'others',\n",
       " 'control',\n",
       " 'within',\n",
       " 'large',\n",
       " 'economy',\n",
       " 'press',\n",
       " 'agency',\n",
       " 'water',\n",
       " 'died',\n",
       " 'career',\n",
       " 'making',\n",
       " '...',\n",
       " 'deal',\n",
       " 'attack',\n",
       " 'side',\n",
       " 'seven',\n",
       " 'better',\n",
       " 'less',\n",
       " 'september',\n",
       " 'once',\n",
       " 'clinton',\n",
       " 'main',\n",
       " 'due',\n",
       " 'committee',\n",
       " 'building',\n",
       " 'conference',\n",
       " 'club',\n",
       " 'january',\n",
       " 'decision',\n",
       " 'stock',\n",
       " 'america',\n",
       " 'given',\n",
       " 'give',\n",
       " 'often',\n",
       " 'announced',\n",
       " 'television',\n",
       " 'industry',\n",
       " 'order',\n",
       " 'young',\n",
       " \"'ve\",\n",
       " 'palestinian',\n",
       " 'age',\n",
       " 'start',\n",
       " 'administration',\n",
       " 'russian',\n",
       " 'prices',\n",
       " 'round',\n",
       " 'december',\n",
       " 'nations',\n",
       " \"'m\",\n",
       " 'human',\n",
       " 'india',\n",
       " 'defense',\n",
       " 'asked',\n",
       " 'total',\n",
       " 'october',\n",
       " 'players',\n",
       " 'bill',\n",
       " 'important',\n",
       " 'southern',\n",
       " 'move',\n",
       " 'fire',\n",
       " 'population',\n",
       " 'rose',\n",
       " 'november',\n",
       " 'include',\n",
       " 'further',\n",
       " 'nuclear',\n",
       " 'street',\n",
       " 'taken',\n",
       " 'media',\n",
       " 'different',\n",
       " 'issue',\n",
       " 'received',\n",
       " 'secretary',\n",
       " 'return',\n",
       " 'college',\n",
       " 'working',\n",
       " 'community',\n",
       " 'eight',\n",
       " 'groups',\n",
       " 'despite',\n",
       " 'level',\n",
       " 'largest',\n",
       " 'whose',\n",
       " 'attacks',\n",
       " 'germany',\n",
       " 'august',\n",
       " 'change',\n",
       " 'church',\n",
       " 'nation',\n",
       " 'german',\n",
       " 'station',\n",
       " 'london',\n",
       " 'weeks',\n",
       " 'having',\n",
       " '18',\n",
       " 'research',\n",
       " 'black',\n",
       " 'services',\n",
       " 'story',\n",
       " '6',\n",
       " 'europe',\n",
       " 'sales',\n",
       " 'policy',\n",
       " 'visit',\n",
       " 'northern',\n",
       " 'lot',\n",
       " 'across',\n",
       " 'per',\n",
       " 'current',\n",
       " 'board',\n",
       " 'football',\n",
       " 'ministry',\n",
       " 'workers',\n",
       " 'vote',\n",
       " 'book',\n",
       " 'fell',\n",
       " 'seen',\n",
       " 'role',\n",
       " 'students',\n",
       " 'shares',\n",
       " 'iran',\n",
       " 'process',\n",
       " 'agreement',\n",
       " 'quarter',\n",
       " 'full',\n",
       " 'match',\n",
       " 'started',\n",
       " 'growth',\n",
       " 'yet',\n",
       " 'moved',\n",
       " 'possible',\n",
       " 'western',\n",
       " 'special',\n",
       " '100',\n",
       " 'plans',\n",
       " 'interest',\n",
       " 'behind',\n",
       " 'strong',\n",
       " 'england',\n",
       " 'named',\n",
       " 'food',\n",
       " 'period',\n",
       " 'real',\n",
       " 'authorities',\n",
       " 'car',\n",
       " 'term',\n",
       " 'rate',\n",
       " 'race',\n",
       " 'nearly',\n",
       " 'korea',\n",
       " 'enough',\n",
       " 'site',\n",
       " 'opposition',\n",
       " 'keep',\n",
       " '25',\n",
       " 'call',\n",
       " 'future',\n",
       " 'taking',\n",
       " 'island',\n",
       " '2008',\n",
       " '2006',\n",
       " 'road',\n",
       " 'outside',\n",
       " 'really',\n",
       " 'century',\n",
       " 'democratic',\n",
       " 'almost',\n",
       " 'single',\n",
       " 'share',\n",
       " 'leading',\n",
       " 'trying',\n",
       " 'find',\n",
       " 'album',\n",
       " 'senior',\n",
       " 'minutes',\n",
       " 'together',\n",
       " 'congress',\n",
       " 'index',\n",
       " 'australia',\n",
       " 'results',\n",
       " 'hard',\n",
       " 'hours',\n",
       " 'land',\n",
       " 'action',\n",
       " 'higher',\n",
       " 'field',\n",
       " 'cut',\n",
       " 'coach',\n",
       " 'elections',\n",
       " 'san',\n",
       " 'issues',\n",
       " 'executive',\n",
       " 'february',\n",
       " 'production',\n",
       " 'areas',\n",
       " 'river',\n",
       " 'face',\n",
       " 'using',\n",
       " 'japanese',\n",
       " 'province',\n",
       " 'park',\n",
       " 'price',\n",
       " 'commission',\n",
       " 'california',\n",
       " 'father',\n",
       " 'son',\n",
       " 'education',\n",
       " '7',\n",
       " 'village',\n",
       " 'energy',\n",
       " 'shot',\n",
       " 'short',\n",
       " 'africa',\n",
       " 'key',\n",
       " 'red',\n",
       " 'association',\n",
       " 'average',\n",
       " 'pay',\n",
       " 'exchange',\n",
       " 'eu',\n",
       " 'something',\n",
       " 'gave',\n",
       " 'likely',\n",
       " 'player',\n",
       " 'george',\n",
       " '2007',\n",
       " 'victory',\n",
       " '8',\n",
       " 'low',\n",
       " 'things',\n",
       " '2010',\n",
       " 'pakistan',\n",
       " '14',\n",
       " 'post',\n",
       " 'social',\n",
       " 'continue',\n",
       " 'ever',\n",
       " 'look',\n",
       " 'chairman',\n",
       " 'job',\n",
       " '2000',\n",
       " 'soldiers',\n",
       " 'able',\n",
       " 'parliament',\n",
       " 'front',\n",
       " 'himself',\n",
       " 'problems',\n",
       " 'private',\n",
       " 'lower',\n",
       " 'list',\n",
       " 'built',\n",
       " '13',\n",
       " 'efforts',\n",
       " 'dollar',\n",
       " 'miles',\n",
       " 'included',\n",
       " 'radio',\n",
       " 'live',\n",
       " 'form',\n",
       " 'david',\n",
       " 'african',\n",
       " 'increase',\n",
       " 'reports',\n",
       " 'sent',\n",
       " 'fourth',\n",
       " 'always',\n",
       " 'king',\n",
       " '50',\n",
       " 'tax',\n",
       " 'taiwan',\n",
       " 'britain',\n",
       " '16',\n",
       " 'playing',\n",
       " 'title',\n",
       " 'middle',\n",
       " 'meet',\n",
       " 'global',\n",
       " 'wife',\n",
       " '2009',\n",
       " 'position',\n",
       " 'located',\n",
       " 'clear',\n",
       " 'ahead',\n",
       " '2004',\n",
       " '2005',\n",
       " 'iraqi',\n",
       " 'english',\n",
       " 'result',\n",
       " 'release',\n",
       " 'violence',\n",
       " 'goal',\n",
       " 'project',\n",
       " 'closed',\n",
       " 'border',\n",
       " 'body',\n",
       " 'soon',\n",
       " 'crisis',\n",
       " 'division',\n",
       " '&amp;',\n",
       " 'served',\n",
       " 'tour',\n",
       " 'hospital',\n",
       " 'kong',\n",
       " 'test',\n",
       " 'hong',\n",
       " 'u.n.',\n",
       " 'inc.',\n",
       " 'technology',\n",
       " 'believe',\n",
       " 'organization',\n",
       " 'published',\n",
       " 'weapons',\n",
       " 'agreed',\n",
       " 'why',\n",
       " 'nine',\n",
       " 'summer',\n",
       " 'wanted',\n",
       " 'republican',\n",
       " 'act',\n",
       " 'recently',\n",
       " 'texas',\n",
       " 'course',\n",
       " 'problem',\n",
       " 'senate',\n",
       " 'medical',\n",
       " 'un',\n",
       " 'done',\n",
       " 'reached',\n",
       " 'star',\n",
       " 'continued',\n",
       " 'investors',\n",
       " 'living',\n",
       " 'care',\n",
       " 'signed',\n",
       " '17',\n",
       " 'art',\n",
       " 'provide',\n",
       " 'worked',\n",
       " 'presidential',\n",
       " 'gold',\n",
       " 'obama',\n",
       " 'morning',\n",
       " 'dead',\n",
       " 'opened',\n",
       " \"'ll\",\n",
       " 'event',\n",
       " 'previous',\n",
       " 'cost',\n",
       " 'instead',\n",
       " 'canada',\n",
       " 'band',\n",
       " 'teams',\n",
       " 'daily',\n",
       " '2001',\n",
       " 'available',\n",
       " 'drug',\n",
       " 'coming',\n",
       " '2003',\n",
       " 'investment',\n",
       " '’s',\n",
       " 'michael',\n",
       " 'civil',\n",
       " 'woman',\n",
       " 'training',\n",
       " 'appeared',\n",
       " '9',\n",
       " 'involved',\n",
       " 'indian',\n",
       " 'similar',\n",
       " 'situation',\n",
       " '24',\n",
       " 'los',\n",
       " 'running',\n",
       " 'fighting',\n",
       " 'mark',\n",
       " '40',\n",
       " 'trial',\n",
       " 'hold',\n",
       " 'australian',\n",
       " 'thought',\n",
       " '!',\n",
       " 'study',\n",
       " 'fall',\n",
       " 'mother',\n",
       " 'met',\n",
       " 'relations',\n",
       " 'anti',\n",
       " '2002',\n",
       " 'song',\n",
       " 'popular',\n",
       " 'base',\n",
       " 'tv',\n",
       " 'ground',\n",
       " 'markets',\n",
       " 'ii',\n",
       " 'newspaper',\n",
       " 'staff',\n",
       " 'saw',\n",
       " 'hand',\n",
       " 'hope',\n",
       " 'operations',\n",
       " 'pressure',\n",
       " 'americans',\n",
       " 'eastern',\n",
       " 'st.',\n",
       " 'legal',\n",
       " 'asia',\n",
       " 'budget',\n",
       " 'returned',\n",
       " 'considered',\n",
       " 'love',\n",
       " 'wrote',\n",
       " 'stop',\n",
       " 'fight',\n",
       " 'currently',\n",
       " 'charges',\n",
       " 'try',\n",
       " 'aid',\n",
       " 'ended',\n",
       " 'management',\n",
       " 'brought',\n",
       " 'cases',\n",
       " 'decided',\n",
       " 'failed',\n",
       " 'network',\n",
       " 'works',\n",
       " 'gas',\n",
       " 'turned',\n",
       " 'fact',\n",
       " 'vice',\n",
       " 'ca',\n",
       " 'mexico',\n",
       " 'trading',\n",
       " 'especially',\n",
       " 'reporters',\n",
       " 'afghanistan',\n",
       " 'common',\n",
       " 'looking',\n",
       " 'space',\n",
       " 'rates',\n",
       " 'manager',\n",
       " 'loss',\n",
       " '2011',\n",
       " 'justice',\n",
       " 'thousands',\n",
       " 'james',\n",
       " 'rather',\n",
       " 'fund',\n",
       " 'thing',\n",
       " 'republic',\n",
       " 'opening',\n",
       " 'accused',\n",
       " 'winning',\n",
       " 'scored',\n",
       " 'championship',\n",
       " 'example',\n",
       " 'getting',\n",
       " 'biggest',\n",
       " 'performance',\n",
       " 'sports',\n",
       " '1998',\n",
       " 'let',\n",
       " 'allowed',\n",
       " 'schools',\n",
       " 'means',\n",
       " 'turn',\n",
       " 'leave',\n",
       " 'no.',\n",
       " 'robert',\n",
       " 'personal',\n",
       " 'stocks',\n",
       " 'showed',\n",
       " 'light',\n",
       " 'arrested',\n",
       " 'person',\n",
       " 'either',\n",
       " 'offer',\n",
       " 'majority',\n",
       " 'battle',\n",
       " '19',\n",
       " 'class',\n",
       " 'evidence',\n",
       " 'makes',\n",
       " 'society',\n",
       " 'products',\n",
       " 'regional',\n",
       " 'needed',\n",
       " 'stage',\n",
       " 'am',\n",
       " 'doing',\n",
       " 'families',\n",
       " 'construction',\n",
       " 'various',\n",
       " '1996',\n",
       " 'sold',\n",
       " 'independent',\n",
       " 'kind',\n",
       " 'airport',\n",
       " 'paul',\n",
       " 'judge',\n",
       " 'internet',\n",
       " 'movement',\n",
       " 'room',\n",
       " 'followed',\n",
       " 'original',\n",
       " 'angeles',\n",
       " 'italy',\n",
       " '`',\n",
       " 'data',\n",
       " 'comes',\n",
       " 'parties',\n",
       " 'nothing',\n",
       " 'sea',\n",
       " 'bring',\n",
       " '2012',\n",
       " 'annual',\n",
       " 'officer',\n",
       " 'beijing',\n",
       " 'present',\n",
       " 'remain',\n",
       " 'nato',\n",
       " '1999',\n",
       " '22',\n",
       " 'remains',\n",
       " 'allow',\n",
       " 'florida',\n",
       " 'computer',\n",
       " '21',\n",
       " 'contract',\n",
       " 'coast',\n",
       " 'created',\n",
       " 'demand',\n",
       " 'operation',\n",
       " 'events',\n",
       " 'islamic',\n",
       " 'beat',\n",
       " 'analysts',\n",
       " 'interview',\n",
       " 'helped',\n",
       " 'child',\n",
       " 'probably',\n",
       " 'spent',\n",
       " 'asian',\n",
       " 'effort',\n",
       " 'cooperation',\n",
       " 'shows',\n",
       " 'calls',\n",
       " 'investigation',\n",
       " 'lives',\n",
       " 'video',\n",
       " 'yen',\n",
       " 'runs',\n",
       " 'tried',\n",
       " 'bad',\n",
       " 'described',\n",
       " '1994',\n",
       " 'toward',\n",
       " 'written',\n",
       " 'throughout',\n",
       " 'established',\n",
       " 'mission',\n",
       " 'associated',\n",
       " 'buy',\n",
       " 'growing',\n",
       " 'green',\n",
       " 'forward',\n",
       " 'competition',\n",
       " 'poor',\n",
       " 'latest',\n",
       " 'banks',\n",
       " 'question',\n",
       " '1997',\n",
       " 'prison',\n",
       " 'feel',\n",
       " 'attention',\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.load(open('C:/Users/Gavrilov/.keras/datasets/glove_6B/'+name+'_glove_words.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " ',': 1,\n",
       " '.': 2,\n",
       " 'of': 3,\n",
       " 'to': 4,\n",
       " 'and': 5,\n",
       " 'in': 6,\n",
       " 'a': 7,\n",
       " '\"': 8,\n",
       " \"'s\": 9,\n",
       " 'for': 10,\n",
       " '-': 11,\n",
       " 'that': 12,\n",
       " 'on': 13,\n",
       " 'is': 14,\n",
       " 'was': 15,\n",
       " 'said': 16,\n",
       " 'with': 17,\n",
       " 'he': 18,\n",
       " 'as': 19,\n",
       " 'it': 20,\n",
       " 'by': 21,\n",
       " 'at': 22,\n",
       " '(': 23,\n",
       " ')': 24,\n",
       " 'from': 25,\n",
       " 'his': 26,\n",
       " \"''\": 27,\n",
       " '``': 28,\n",
       " 'an': 29,\n",
       " 'be': 30,\n",
       " 'has': 31,\n",
       " 'are': 32,\n",
       " 'have': 33,\n",
       " 'but': 34,\n",
       " 'were': 35,\n",
       " 'not': 36,\n",
       " 'this': 37,\n",
       " 'who': 38,\n",
       " 'they': 39,\n",
       " 'had': 40,\n",
       " 'i': 41,\n",
       " 'which': 42,\n",
       " 'will': 43,\n",
       " 'their': 44,\n",
       " ':': 45,\n",
       " 'or': 46,\n",
       " 'its': 47,\n",
       " 'one': 48,\n",
       " 'after': 49,\n",
       " 'new': 50,\n",
       " 'been': 51,\n",
       " 'also': 52,\n",
       " 'we': 53,\n",
       " 'would': 54,\n",
       " 'two': 55,\n",
       " 'more': 56,\n",
       " \"'\": 57,\n",
       " 'first': 58,\n",
       " 'about': 59,\n",
       " 'up': 60,\n",
       " 'when': 61,\n",
       " 'year': 62,\n",
       " 'there': 63,\n",
       " 'all': 64,\n",
       " '--': 65,\n",
       " 'out': 66,\n",
       " 'she': 67,\n",
       " 'other': 68,\n",
       " 'people': 69,\n",
       " \"n't\": 70,\n",
       " 'her': 71,\n",
       " 'percent': 72,\n",
       " 'than': 73,\n",
       " 'over': 74,\n",
       " 'into': 75,\n",
       " 'last': 76,\n",
       " 'some': 77,\n",
       " 'government': 78,\n",
       " 'time': 79,\n",
       " '$': 80,\n",
       " 'you': 81,\n",
       " 'years': 82,\n",
       " 'if': 83,\n",
       " 'no': 84,\n",
       " 'world': 85,\n",
       " 'can': 86,\n",
       " 'three': 87,\n",
       " 'do': 88,\n",
       " ';': 89,\n",
       " 'president': 90,\n",
       " 'only': 91,\n",
       " 'state': 92,\n",
       " 'million': 93,\n",
       " 'could': 94,\n",
       " 'us': 95,\n",
       " 'most': 96,\n",
       " '_': 97,\n",
       " 'against': 98,\n",
       " 'u.s.': 99,\n",
       " 'so': 100,\n",
       " 'them': 101,\n",
       " 'what': 102,\n",
       " 'him': 103,\n",
       " 'united': 104,\n",
       " 'during': 105,\n",
       " 'before': 106,\n",
       " 'may': 107,\n",
       " 'since': 108,\n",
       " 'many': 109,\n",
       " 'while': 110,\n",
       " 'where': 111,\n",
       " 'states': 112,\n",
       " 'because': 113,\n",
       " 'now': 114,\n",
       " 'city': 115,\n",
       " 'made': 116,\n",
       " 'like': 117,\n",
       " 'between': 118,\n",
       " 'did': 119,\n",
       " 'just': 120,\n",
       " 'national': 121,\n",
       " 'day': 122,\n",
       " 'country': 123,\n",
       " 'under': 124,\n",
       " 'such': 125,\n",
       " 'second': 126,\n",
       " 'then': 127,\n",
       " 'company': 128,\n",
       " 'group': 129,\n",
       " 'any': 130,\n",
       " 'through': 131,\n",
       " 'china': 132,\n",
       " 'four': 133,\n",
       " 'being': 134,\n",
       " 'down': 135,\n",
       " 'war': 136,\n",
       " 'back': 137,\n",
       " 'off': 138,\n",
       " 'south': 139,\n",
       " 'american': 140,\n",
       " 'minister': 141,\n",
       " 'police': 142,\n",
       " 'well': 143,\n",
       " 'including': 144,\n",
       " 'team': 145,\n",
       " 'international': 146,\n",
       " 'week': 147,\n",
       " 'officials': 148,\n",
       " 'still': 149,\n",
       " 'both': 150,\n",
       " 'even': 151,\n",
       " 'high': 152,\n",
       " 'part': 153,\n",
       " 'told': 154,\n",
       " 'those': 155,\n",
       " 'end': 156,\n",
       " 'former': 157,\n",
       " 'these': 158,\n",
       " 'make': 159,\n",
       " 'billion': 160,\n",
       " 'work': 161,\n",
       " 'our': 162,\n",
       " 'home': 163,\n",
       " 'school': 164,\n",
       " 'party': 165,\n",
       " 'house': 166,\n",
       " 'old': 167,\n",
       " 'later': 168,\n",
       " 'get': 169,\n",
       " 'another': 170,\n",
       " 'tuesday': 171,\n",
       " 'news': 172,\n",
       " 'long': 173,\n",
       " 'five': 174,\n",
       " 'called': 175,\n",
       " '1': 176,\n",
       " 'wednesday': 177,\n",
       " 'military': 178,\n",
       " 'way': 179,\n",
       " 'used': 180,\n",
       " 'much': 181,\n",
       " 'next': 182,\n",
       " 'monday': 183,\n",
       " 'thursday': 184,\n",
       " 'friday': 185,\n",
       " 'game': 186,\n",
       " 'here': 187,\n",
       " '?': 188,\n",
       " 'should': 189,\n",
       " 'take': 190,\n",
       " 'very': 191,\n",
       " 'my': 192,\n",
       " 'north': 193,\n",
       " 'security': 194,\n",
       " 'season': 195,\n",
       " 'york': 196,\n",
       " 'how': 197,\n",
       " 'public': 198,\n",
       " 'early': 199,\n",
       " 'according': 200,\n",
       " 'several': 201,\n",
       " 'court': 202,\n",
       " 'say': 203,\n",
       " 'around': 204,\n",
       " 'foreign': 205,\n",
       " '10': 206,\n",
       " 'until': 207,\n",
       " 'set': 208,\n",
       " 'political': 209,\n",
       " 'says': 210,\n",
       " 'market': 211,\n",
       " 'however': 212,\n",
       " 'family': 213,\n",
       " 'life': 214,\n",
       " 'same': 215,\n",
       " 'general': 216,\n",
       " '–': 217,\n",
       " 'left': 218,\n",
       " 'good': 219,\n",
       " 'top': 220,\n",
       " 'university': 221,\n",
       " 'going': 222,\n",
       " 'number': 223,\n",
       " 'major': 224,\n",
       " 'known': 225,\n",
       " 'points': 226,\n",
       " 'won': 227,\n",
       " 'six': 228,\n",
       " 'month': 229,\n",
       " 'dollars': 230,\n",
       " 'bank': 231,\n",
       " '2': 232,\n",
       " 'iraq': 233,\n",
       " 'use': 234,\n",
       " 'members': 235,\n",
       " 'each': 236,\n",
       " 'area': 237,\n",
       " 'found': 238,\n",
       " 'official': 239,\n",
       " 'sunday': 240,\n",
       " 'place': 241,\n",
       " 'go': 242,\n",
       " 'based': 243,\n",
       " 'among': 244,\n",
       " 'third': 245,\n",
       " 'times': 246,\n",
       " 'took': 247,\n",
       " 'right': 248,\n",
       " 'days': 249,\n",
       " 'local': 250,\n",
       " 'economic': 251,\n",
       " 'countries': 252,\n",
       " 'see': 253,\n",
       " 'best': 254,\n",
       " 'report': 255,\n",
       " 'killed': 256,\n",
       " 'held': 257,\n",
       " 'business': 258,\n",
       " 'west': 259,\n",
       " 'does': 260,\n",
       " 'own': 261,\n",
       " '%': 262,\n",
       " 'came': 263,\n",
       " 'law': 264,\n",
       " 'months': 265,\n",
       " 'women': 266,\n",
       " \"'re\": 267,\n",
       " 'power': 268,\n",
       " 'think': 269,\n",
       " 'service': 270,\n",
       " 'children': 271,\n",
       " 'bush': 272,\n",
       " 'show': 273,\n",
       " '/': 274,\n",
       " 'help': 275,\n",
       " 'chief': 276,\n",
       " 'saturday': 277,\n",
       " 'system': 278,\n",
       " 'john': 279,\n",
       " 'support': 280,\n",
       " 'series': 281,\n",
       " 'play': 282,\n",
       " 'office': 283,\n",
       " 'following': 284,\n",
       " 'me': 285,\n",
       " 'meeting': 286,\n",
       " 'expected': 287,\n",
       " 'late': 288,\n",
       " 'washington': 289,\n",
       " 'games': 290,\n",
       " 'european': 291,\n",
       " 'league': 292,\n",
       " 'reported': 293,\n",
       " 'final': 294,\n",
       " 'added': 295,\n",
       " 'without': 296,\n",
       " 'british': 297,\n",
       " 'white': 298,\n",
       " 'history': 299,\n",
       " 'man': 300,\n",
       " 'men': 301,\n",
       " 'became': 302,\n",
       " 'want': 303,\n",
       " 'march': 304,\n",
       " 'case': 305,\n",
       " 'few': 306,\n",
       " 'run': 307,\n",
       " 'money': 308,\n",
       " 'began': 309,\n",
       " 'open': 310,\n",
       " 'name': 311,\n",
       " 'trade': 312,\n",
       " 'center': 313,\n",
       " '3': 314,\n",
       " 'israel': 315,\n",
       " 'oil': 316,\n",
       " 'too': 317,\n",
       " 'al': 318,\n",
       " 'film': 319,\n",
       " 'win': 320,\n",
       " 'led': 321,\n",
       " 'east': 322,\n",
       " 'central': 323,\n",
       " '20': 324,\n",
       " 'air': 325,\n",
       " 'come': 326,\n",
       " 'chinese': 327,\n",
       " 'town': 328,\n",
       " 'leader': 329,\n",
       " 'army': 330,\n",
       " 'line': 331,\n",
       " 'never': 332,\n",
       " 'little': 333,\n",
       " 'played': 334,\n",
       " 'prime': 335,\n",
       " 'death': 336,\n",
       " 'companies': 337,\n",
       " 'least': 338,\n",
       " 'put': 339,\n",
       " 'forces': 340,\n",
       " 'past': 341,\n",
       " 'de': 342,\n",
       " 'half': 343,\n",
       " 'june': 344,\n",
       " 'saying': 345,\n",
       " 'know': 346,\n",
       " 'federal': 347,\n",
       " 'french': 348,\n",
       " 'peace': 349,\n",
       " 'earlier': 350,\n",
       " 'capital': 351,\n",
       " 'force': 352,\n",
       " 'great': 353,\n",
       " 'union': 354,\n",
       " 'near': 355,\n",
       " 'released': 356,\n",
       " 'small': 357,\n",
       " 'department': 358,\n",
       " 'every': 359,\n",
       " 'health': 360,\n",
       " 'japan': 361,\n",
       " 'head': 362,\n",
       " 'ago': 363,\n",
       " 'night': 364,\n",
       " 'big': 365,\n",
       " 'cup': 366,\n",
       " 'election': 367,\n",
       " 'region': 368,\n",
       " 'director': 369,\n",
       " 'talks': 370,\n",
       " 'program': 371,\n",
       " 'far': 372,\n",
       " 'today': 373,\n",
       " 'statement': 374,\n",
       " 'july': 375,\n",
       " 'although': 376,\n",
       " 'district': 377,\n",
       " 'again': 378,\n",
       " 'born': 379,\n",
       " 'development': 380,\n",
       " 'leaders': 381,\n",
       " 'council': 382,\n",
       " 'close': 383,\n",
       " 'record': 384,\n",
       " 'along': 385,\n",
       " 'county': 386,\n",
       " 'france': 387,\n",
       " 'went': 388,\n",
       " 'point': 389,\n",
       " 'must': 390,\n",
       " 'spokesman': 391,\n",
       " 'your': 392,\n",
       " 'member': 393,\n",
       " 'plan': 394,\n",
       " 'financial': 395,\n",
       " 'april': 396,\n",
       " 'recent': 397,\n",
       " 'campaign': 398,\n",
       " 'become': 399,\n",
       " 'troops': 400,\n",
       " 'whether': 401,\n",
       " 'lost': 402,\n",
       " 'music': 403,\n",
       " '15': 404,\n",
       " 'got': 405,\n",
       " 'israeli': 406,\n",
       " '30': 407,\n",
       " 'need': 408,\n",
       " '4': 409,\n",
       " 'lead': 410,\n",
       " 'already': 411,\n",
       " 'russia': 412,\n",
       " 'though': 413,\n",
       " 'might': 414,\n",
       " 'free': 415,\n",
       " 'hit': 416,\n",
       " 'rights': 417,\n",
       " '11': 418,\n",
       " 'information': 419,\n",
       " 'away': 420,\n",
       " '12': 421,\n",
       " '5': 422,\n",
       " 'others': 423,\n",
       " 'control': 424,\n",
       " 'within': 425,\n",
       " 'large': 426,\n",
       " 'economy': 427,\n",
       " 'press': 428,\n",
       " 'agency': 429,\n",
       " 'water': 430,\n",
       " 'died': 431,\n",
       " 'career': 432,\n",
       " 'making': 433,\n",
       " '...': 434,\n",
       " 'deal': 435,\n",
       " 'attack': 436,\n",
       " 'side': 437,\n",
       " 'seven': 438,\n",
       " 'better': 439,\n",
       " 'less': 440,\n",
       " 'september': 441,\n",
       " 'once': 442,\n",
       " 'clinton': 443,\n",
       " 'main': 444,\n",
       " 'due': 445,\n",
       " 'committee': 446,\n",
       " 'building': 447,\n",
       " 'conference': 448,\n",
       " 'club': 449,\n",
       " 'january': 450,\n",
       " 'decision': 451,\n",
       " 'stock': 452,\n",
       " 'america': 453,\n",
       " 'given': 454,\n",
       " 'give': 455,\n",
       " 'often': 456,\n",
       " 'announced': 457,\n",
       " 'television': 458,\n",
       " 'industry': 459,\n",
       " 'order': 460,\n",
       " 'young': 461,\n",
       " \"'ve\": 462,\n",
       " 'palestinian': 463,\n",
       " 'age': 464,\n",
       " 'start': 465,\n",
       " 'administration': 466,\n",
       " 'russian': 467,\n",
       " 'prices': 468,\n",
       " 'round': 469,\n",
       " 'december': 470,\n",
       " 'nations': 471,\n",
       " \"'m\": 472,\n",
       " 'human': 473,\n",
       " 'india': 474,\n",
       " 'defense': 475,\n",
       " 'asked': 476,\n",
       " 'total': 477,\n",
       " 'october': 478,\n",
       " 'players': 479,\n",
       " 'bill': 480,\n",
       " 'important': 481,\n",
       " 'southern': 482,\n",
       " 'move': 483,\n",
       " 'fire': 484,\n",
       " 'population': 485,\n",
       " 'rose': 486,\n",
       " 'november': 487,\n",
       " 'include': 488,\n",
       " 'further': 489,\n",
       " 'nuclear': 490,\n",
       " 'street': 491,\n",
       " 'taken': 492,\n",
       " 'media': 493,\n",
       " 'different': 494,\n",
       " 'issue': 495,\n",
       " 'received': 496,\n",
       " 'secretary': 497,\n",
       " 'return': 498,\n",
       " 'college': 499,\n",
       " 'working': 500,\n",
       " 'community': 501,\n",
       " 'eight': 502,\n",
       " 'groups': 503,\n",
       " 'despite': 504,\n",
       " 'level': 505,\n",
       " 'largest': 506,\n",
       " 'whose': 507,\n",
       " 'attacks': 508,\n",
       " 'germany': 509,\n",
       " 'august': 510,\n",
       " 'change': 511,\n",
       " 'church': 512,\n",
       " 'nation': 513,\n",
       " 'german': 514,\n",
       " 'station': 515,\n",
       " 'london': 516,\n",
       " 'weeks': 517,\n",
       " 'having': 518,\n",
       " '18': 519,\n",
       " 'research': 520,\n",
       " 'black': 521,\n",
       " 'services': 522,\n",
       " 'story': 523,\n",
       " '6': 524,\n",
       " 'europe': 525,\n",
       " 'sales': 526,\n",
       " 'policy': 527,\n",
       " 'visit': 528,\n",
       " 'northern': 529,\n",
       " 'lot': 530,\n",
       " 'across': 531,\n",
       " 'per': 532,\n",
       " 'current': 533,\n",
       " 'board': 534,\n",
       " 'football': 535,\n",
       " 'ministry': 536,\n",
       " 'workers': 537,\n",
       " 'vote': 538,\n",
       " 'book': 539,\n",
       " 'fell': 540,\n",
       " 'seen': 541,\n",
       " 'role': 542,\n",
       " 'students': 543,\n",
       " 'shares': 544,\n",
       " 'iran': 545,\n",
       " 'process': 546,\n",
       " 'agreement': 547,\n",
       " 'quarter': 548,\n",
       " 'full': 549,\n",
       " 'match': 550,\n",
       " 'started': 551,\n",
       " 'growth': 552,\n",
       " 'yet': 553,\n",
       " 'moved': 554,\n",
       " 'possible': 555,\n",
       " 'western': 556,\n",
       " 'special': 557,\n",
       " '100': 558,\n",
       " 'plans': 559,\n",
       " 'interest': 560,\n",
       " 'behind': 561,\n",
       " 'strong': 562,\n",
       " 'england': 563,\n",
       " 'named': 564,\n",
       " 'food': 565,\n",
       " 'period': 566,\n",
       " 'real': 567,\n",
       " 'authorities': 568,\n",
       " 'car': 569,\n",
       " 'term': 570,\n",
       " 'rate': 571,\n",
       " 'race': 572,\n",
       " 'nearly': 573,\n",
       " 'korea': 574,\n",
       " 'enough': 575,\n",
       " 'site': 576,\n",
       " 'opposition': 577,\n",
       " 'keep': 578,\n",
       " '25': 579,\n",
       " 'call': 580,\n",
       " 'future': 581,\n",
       " 'taking': 582,\n",
       " 'island': 583,\n",
       " '2008': 584,\n",
       " '2006': 585,\n",
       " 'road': 586,\n",
       " 'outside': 587,\n",
       " 'really': 588,\n",
       " 'century': 589,\n",
       " 'democratic': 590,\n",
       " 'almost': 591,\n",
       " 'single': 592,\n",
       " 'share': 593,\n",
       " 'leading': 594,\n",
       " 'trying': 595,\n",
       " 'find': 596,\n",
       " 'album': 597,\n",
       " 'senior': 598,\n",
       " 'minutes': 599,\n",
       " 'together': 600,\n",
       " 'congress': 601,\n",
       " 'index': 602,\n",
       " 'australia': 603,\n",
       " 'results': 604,\n",
       " 'hard': 605,\n",
       " 'hours': 606,\n",
       " 'land': 607,\n",
       " 'action': 608,\n",
       " 'higher': 609,\n",
       " 'field': 610,\n",
       " 'cut': 611,\n",
       " 'coach': 612,\n",
       " 'elections': 613,\n",
       " 'san': 614,\n",
       " 'issues': 615,\n",
       " 'executive': 616,\n",
       " 'february': 617,\n",
       " 'production': 618,\n",
       " 'areas': 619,\n",
       " 'river': 620,\n",
       " 'face': 621,\n",
       " 'using': 622,\n",
       " 'japanese': 623,\n",
       " 'province': 624,\n",
       " 'park': 625,\n",
       " 'price': 626,\n",
       " 'commission': 627,\n",
       " 'california': 628,\n",
       " 'father': 629,\n",
       " 'son': 630,\n",
       " 'education': 631,\n",
       " '7': 632,\n",
       " 'village': 633,\n",
       " 'energy': 634,\n",
       " 'shot': 635,\n",
       " 'short': 636,\n",
       " 'africa': 637,\n",
       " 'key': 638,\n",
       " 'red': 639,\n",
       " 'association': 640,\n",
       " 'average': 641,\n",
       " 'pay': 642,\n",
       " 'exchange': 643,\n",
       " 'eu': 644,\n",
       " 'something': 645,\n",
       " 'gave': 646,\n",
       " 'likely': 647,\n",
       " 'player': 648,\n",
       " 'george': 649,\n",
       " '2007': 650,\n",
       " 'victory': 651,\n",
       " '8': 652,\n",
       " 'low': 653,\n",
       " 'things': 654,\n",
       " '2010': 655,\n",
       " 'pakistan': 656,\n",
       " '14': 657,\n",
       " 'post': 658,\n",
       " 'social': 659,\n",
       " 'continue': 660,\n",
       " 'ever': 661,\n",
       " 'look': 662,\n",
       " 'chairman': 663,\n",
       " 'job': 664,\n",
       " '2000': 665,\n",
       " 'soldiers': 666,\n",
       " 'able': 667,\n",
       " 'parliament': 668,\n",
       " 'front': 669,\n",
       " 'himself': 670,\n",
       " 'problems': 671,\n",
       " 'private': 672,\n",
       " 'lower': 673,\n",
       " 'list': 674,\n",
       " 'built': 675,\n",
       " '13': 676,\n",
       " 'efforts': 677,\n",
       " 'dollar': 678,\n",
       " 'miles': 679,\n",
       " 'included': 680,\n",
       " 'radio': 681,\n",
       " 'live': 682,\n",
       " 'form': 683,\n",
       " 'david': 684,\n",
       " 'african': 685,\n",
       " 'increase': 686,\n",
       " 'reports': 687,\n",
       " 'sent': 688,\n",
       " 'fourth': 689,\n",
       " 'always': 690,\n",
       " 'king': 691,\n",
       " '50': 692,\n",
       " 'tax': 693,\n",
       " 'taiwan': 694,\n",
       " 'britain': 695,\n",
       " '16': 696,\n",
       " 'playing': 697,\n",
       " 'title': 698,\n",
       " 'middle': 699,\n",
       " 'meet': 700,\n",
       " 'global': 701,\n",
       " 'wife': 702,\n",
       " '2009': 703,\n",
       " 'position': 704,\n",
       " 'located': 705,\n",
       " 'clear': 706,\n",
       " 'ahead': 707,\n",
       " '2004': 708,\n",
       " '2005': 709,\n",
       " 'iraqi': 710,\n",
       " 'english': 711,\n",
       " 'result': 712,\n",
       " 'release': 713,\n",
       " 'violence': 714,\n",
       " 'goal': 715,\n",
       " 'project': 716,\n",
       " 'closed': 717,\n",
       " 'border': 718,\n",
       " 'body': 719,\n",
       " 'soon': 720,\n",
       " 'crisis': 721,\n",
       " 'division': 722,\n",
       " '&amp;': 723,\n",
       " 'served': 724,\n",
       " 'tour': 725,\n",
       " 'hospital': 726,\n",
       " 'kong': 727,\n",
       " 'test': 728,\n",
       " 'hong': 729,\n",
       " 'u.n.': 730,\n",
       " 'inc.': 731,\n",
       " 'technology': 732,\n",
       " 'believe': 733,\n",
       " 'organization': 734,\n",
       " 'published': 735,\n",
       " 'weapons': 736,\n",
       " 'agreed': 737,\n",
       " 'why': 738,\n",
       " 'nine': 739,\n",
       " 'summer': 740,\n",
       " 'wanted': 741,\n",
       " 'republican': 742,\n",
       " 'act': 743,\n",
       " 'recently': 744,\n",
       " 'texas': 745,\n",
       " 'course': 746,\n",
       " 'problem': 747,\n",
       " 'senate': 748,\n",
       " 'medical': 749,\n",
       " 'un': 750,\n",
       " 'done': 751,\n",
       " 'reached': 752,\n",
       " 'star': 753,\n",
       " 'continued': 754,\n",
       " 'investors': 755,\n",
       " 'living': 756,\n",
       " 'care': 757,\n",
       " 'signed': 758,\n",
       " '17': 759,\n",
       " 'art': 760,\n",
       " 'provide': 761,\n",
       " 'worked': 762,\n",
       " 'presidential': 763,\n",
       " 'gold': 764,\n",
       " 'obama': 765,\n",
       " 'morning': 766,\n",
       " 'dead': 767,\n",
       " 'opened': 768,\n",
       " \"'ll\": 769,\n",
       " 'event': 770,\n",
       " 'previous': 771,\n",
       " 'cost': 772,\n",
       " 'instead': 773,\n",
       " 'canada': 774,\n",
       " 'band': 775,\n",
       " 'teams': 776,\n",
       " 'daily': 777,\n",
       " '2001': 778,\n",
       " 'available': 779,\n",
       " 'drug': 780,\n",
       " 'coming': 781,\n",
       " '2003': 782,\n",
       " 'investment': 783,\n",
       " '’s': 784,\n",
       " 'michael': 785,\n",
       " 'civil': 786,\n",
       " 'woman': 787,\n",
       " 'training': 788,\n",
       " 'appeared': 789,\n",
       " '9': 790,\n",
       " 'involved': 791,\n",
       " 'indian': 792,\n",
       " 'similar': 793,\n",
       " 'situation': 794,\n",
       " '24': 795,\n",
       " 'los': 796,\n",
       " 'running': 797,\n",
       " 'fighting': 798,\n",
       " 'mark': 799,\n",
       " '40': 800,\n",
       " 'trial': 801,\n",
       " 'hold': 802,\n",
       " 'australian': 803,\n",
       " 'thought': 804,\n",
       " '!': 805,\n",
       " 'study': 806,\n",
       " 'fall': 807,\n",
       " 'mother': 808,\n",
       " 'met': 809,\n",
       " 'relations': 810,\n",
       " 'anti': 811,\n",
       " '2002': 812,\n",
       " 'song': 813,\n",
       " 'popular': 814,\n",
       " 'base': 815,\n",
       " 'tv': 816,\n",
       " 'ground': 817,\n",
       " 'markets': 818,\n",
       " 'ii': 819,\n",
       " 'newspaper': 820,\n",
       " 'staff': 821,\n",
       " 'saw': 822,\n",
       " 'hand': 823,\n",
       " 'hope': 824,\n",
       " 'operations': 825,\n",
       " 'pressure': 826,\n",
       " 'americans': 827,\n",
       " 'eastern': 828,\n",
       " 'st.': 829,\n",
       " 'legal': 830,\n",
       " 'asia': 831,\n",
       " 'budget': 832,\n",
       " 'returned': 833,\n",
       " 'considered': 834,\n",
       " 'love': 835,\n",
       " 'wrote': 836,\n",
       " 'stop': 837,\n",
       " 'fight': 838,\n",
       " 'currently': 839,\n",
       " 'charges': 840,\n",
       " 'try': 841,\n",
       " 'aid': 842,\n",
       " 'ended': 843,\n",
       " 'management': 844,\n",
       " 'brought': 845,\n",
       " 'cases': 846,\n",
       " 'decided': 847,\n",
       " 'failed': 848,\n",
       " 'network': 849,\n",
       " 'works': 850,\n",
       " 'gas': 851,\n",
       " 'turned': 852,\n",
       " 'fact': 853,\n",
       " 'vice': 854,\n",
       " 'ca': 855,\n",
       " 'mexico': 856,\n",
       " 'trading': 857,\n",
       " 'especially': 858,\n",
       " 'reporters': 859,\n",
       " 'afghanistan': 860,\n",
       " 'common': 861,\n",
       " 'looking': 862,\n",
       " 'space': 863,\n",
       " 'rates': 864,\n",
       " 'manager': 865,\n",
       " 'loss': 866,\n",
       " '2011': 867,\n",
       " 'justice': 868,\n",
       " 'thousands': 869,\n",
       " 'james': 870,\n",
       " 'rather': 871,\n",
       " 'fund': 872,\n",
       " 'thing': 873,\n",
       " 'republic': 874,\n",
       " 'opening': 875,\n",
       " 'accused': 876,\n",
       " 'winning': 877,\n",
       " 'scored': 878,\n",
       " 'championship': 879,\n",
       " 'example': 880,\n",
       " 'getting': 881,\n",
       " 'biggest': 882,\n",
       " 'performance': 883,\n",
       " 'sports': 884,\n",
       " '1998': 885,\n",
       " 'let': 886,\n",
       " 'allowed': 887,\n",
       " 'schools': 888,\n",
       " 'means': 889,\n",
       " 'turn': 890,\n",
       " 'leave': 891,\n",
       " 'no.': 892,\n",
       " 'robert': 893,\n",
       " 'personal': 894,\n",
       " 'stocks': 895,\n",
       " 'showed': 896,\n",
       " 'light': 897,\n",
       " 'arrested': 898,\n",
       " 'person': 899,\n",
       " 'either': 900,\n",
       " 'offer': 901,\n",
       " 'majority': 902,\n",
       " 'battle': 903,\n",
       " '19': 904,\n",
       " 'class': 905,\n",
       " 'evidence': 906,\n",
       " 'makes': 907,\n",
       " 'society': 908,\n",
       " 'products': 909,\n",
       " 'regional': 910,\n",
       " 'needed': 911,\n",
       " 'stage': 912,\n",
       " 'am': 913,\n",
       " 'doing': 914,\n",
       " 'families': 915,\n",
       " 'construction': 916,\n",
       " 'various': 917,\n",
       " '1996': 918,\n",
       " 'sold': 919,\n",
       " 'independent': 920,\n",
       " 'kind': 921,\n",
       " 'airport': 922,\n",
       " 'paul': 923,\n",
       " 'judge': 924,\n",
       " 'internet': 925,\n",
       " 'movement': 926,\n",
       " 'room': 927,\n",
       " 'followed': 928,\n",
       " 'original': 929,\n",
       " 'angeles': 930,\n",
       " 'italy': 931,\n",
       " '`': 932,\n",
       " 'data': 933,\n",
       " 'comes': 934,\n",
       " 'parties': 935,\n",
       " 'nothing': 936,\n",
       " 'sea': 937,\n",
       " 'bring': 938,\n",
       " '2012': 939,\n",
       " 'annual': 940,\n",
       " 'officer': 941,\n",
       " 'beijing': 942,\n",
       " 'present': 943,\n",
       " 'remain': 944,\n",
       " 'nato': 945,\n",
       " '1999': 946,\n",
       " '22': 947,\n",
       " 'remains': 948,\n",
       " 'allow': 949,\n",
       " 'florida': 950,\n",
       " 'computer': 951,\n",
       " '21': 952,\n",
       " 'contract': 953,\n",
       " 'coast': 954,\n",
       " 'created': 955,\n",
       " 'demand': 956,\n",
       " 'operation': 957,\n",
       " 'events': 958,\n",
       " 'islamic': 959,\n",
       " 'beat': 960,\n",
       " 'analysts': 961,\n",
       " 'interview': 962,\n",
       " 'helped': 963,\n",
       " 'child': 964,\n",
       " 'probably': 965,\n",
       " 'spent': 966,\n",
       " 'asian': 967,\n",
       " 'effort': 968,\n",
       " 'cooperation': 969,\n",
       " 'shows': 970,\n",
       " 'calls': 971,\n",
       " 'investigation': 972,\n",
       " 'lives': 973,\n",
       " 'video': 974,\n",
       " 'yen': 975,\n",
       " 'runs': 976,\n",
       " 'tried': 977,\n",
       " 'bad': 978,\n",
       " 'described': 979,\n",
       " '1994': 980,\n",
       " 'toward': 981,\n",
       " 'written': 982,\n",
       " 'throughout': 983,\n",
       " 'established': 984,\n",
       " 'mission': 985,\n",
       " 'associated': 986,\n",
       " 'buy': 987,\n",
       " 'growing': 988,\n",
       " 'green': 989,\n",
       " 'forward': 990,\n",
       " 'competition': 991,\n",
       " 'poor': 992,\n",
       " 'latest': 993,\n",
       " 'banks': 994,\n",
       " 'question': 995,\n",
       " '1997': 996,\n",
       " 'prison': 997,\n",
       " 'feel': 998,\n",
       " 'attention': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.load(open('C:/Users/Gavrilov/.keras/datasets/glove_6B/'+name+'_glove_words_idx.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41800001,  0.24968   , -0.41242   , ..., -0.18411   ,\n",
       "        -0.11514   , -0.78580999],\n",
       "       [ 0.013441  ,  0.23682   , -0.16899   , ..., -0.56656998,\n",
       "         0.044691  ,  0.30392   ],\n",
       "       [ 0.15164   ,  0.30177   , -0.16763   , ..., -0.35652   ,\n",
       "         0.016413  ,  0.10216   ],\n",
       "       ..., \n",
       "       [-0.51181   ,  0.058706  ,  1.09130001, ..., -0.25003001,\n",
       "        -1.125     ,  1.58630002],\n",
       "       [-0.75897998, -0.47426   ,  0.47369999, ...,  0.78953999,\n",
       "        -0.014116  ,  0.64480001],\n",
       "       [ 0.072617  , -0.51393002,  0.47279999, ..., -0.18907   ,\n",
       "        -0.59021002,  0.55558997]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('C:/Users/Gavrilov/.keras/datasets/glove_6B/'+name+'_glove_vecs'+'.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding matrix - extracting indices and vectors from Glove regarding words in IMDb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fact = glove_vecs.shape[1] #getting value, which is same as dimensions of Glove vectors (amount of floats in words array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = np.zeros((vocab_size, n_fact)) #create an array of zeros \n",
    "#creating a numpy array 2D tempalate with number of axes values same as our vocab_size () and dimentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 50),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape, emb[1] #is shape of our future array shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400000, 50),\n",
       " array([ 0.013441  ,  0.23682   , -0.16899   ,  0.40950999,  0.63812   ,\n",
       "         0.47709   , -0.42851999, -0.55641001, -0.36399999, -0.23938   ,\n",
       "         0.13000999, -0.063734  , -0.39574999, -0.48162001,  0.23291001,\n",
       "         0.090201  , -0.13324   ,  0.078639  , -0.41633999, -0.15428001,\n",
       "         0.10068   ,  0.48890999,  0.31226   , -0.1252    , -0.037512  ,\n",
       "        -1.51789999,  0.12612   , -0.02442   , -0.042961  , -0.28351   ,\n",
       "         3.54159999, -0.11956   , -0.014533  , -0.1499    ,  0.21864   ,\n",
       "        -0.33412001, -0.13872001,  0.31806001,  0.70358002,  0.44858   ,\n",
       "        -0.080262  ,  0.63002998,  0.32111001, -0.46765   ,  0.22786   ,\n",
       "         0.36034   , -0.37818   , -0.56656998,  0.044691  ,  0.30392   ], dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vecs.shape, glove_vecs[1] #is shape of exist Glove numpy array shape, from wich we will chose our vectors for words in our vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in range(1, len(emb)): #mapping words vectors from Glove to IMDb words\n",
    "    word = dataset_idx_words[num]\n",
    "    if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word): #bool to find if word match with all variances it could appear (which can happen in Glove) using regular expression (see http://www.regular-expressions.info/python.html)\n",
    "        temp_idx = glove_words_idx[word] #getting index of word in Glove dictionary\n",
    "        emb[num] = glove_vecs[temp_idx] #assigning of created emb matrix of given temp_idx dimensions with relevant values from Glove word embeddings \n",
    "    else: #if no match identified (if we can't find a match in Glove, will just assign to random value)\n",
    "        emb[num] = np.random.normal(scale=0.6, size=(n_fact,)) #have had to create random embedding, because sometimes the word we've lokked in IMDb didn't exist in Glove (for example, words with apostrophe 's')\n",
    "        # basically whole preprocess with Glove is only needed for extracting vectors of words, wich then becomes part of matrix embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.18000013e-01,   2.49679998e-01,  -4.12420005e-01,\n",
       "         1.21699996e-01,   3.45270008e-01,  -4.44569997e-02,\n",
       "        -4.96879995e-01,  -1.78619996e-01,  -6.60229998e-04,\n",
       "        -6.56599998e-01,   2.78430015e-01,  -1.47670001e-01,\n",
       "        -5.56770027e-01,   1.46579996e-01,  -9.50950012e-03,\n",
       "         1.16579998e-02,   1.02040000e-01,  -1.27920002e-01,\n",
       "        -8.44299972e-01,  -1.21809997e-01,  -1.68009996e-02,\n",
       "        -3.32789987e-01,  -1.55200005e-01,  -2.31309995e-01,\n",
       "        -1.91809997e-01,  -1.88230002e+00,  -7.67459989e-01,\n",
       "         9.90509987e-02,  -4.21249986e-01,  -1.95260003e-01,\n",
       "         4.00710011e+00,  -1.85939997e-01,  -5.22870004e-01,\n",
       "        -3.16810012e-01,   5.92130003e-04,   7.44489999e-03,\n",
       "         1.77780002e-01,  -1.58969998e-01,   1.20409997e-02,\n",
       "        -5.42230010e-02,  -2.98709989e-01,  -1.57490000e-01,\n",
       "        -3.47579986e-01,  -4.56370004e-02,  -4.42510009e-01,\n",
       "         1.87849998e-01,   2.78489990e-03,  -1.84110001e-01,\n",
       "        -1.15139998e-01,  -7.85809994e-01])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb[-1] = np.random.normal(scale=0.6, size=(n_fact,)) #this is our \"rare word\" id - we want to randomly initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb/=3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.39333338e-01,   8.32266659e-02,  -1.37473335e-01,\n",
       "         4.05666654e-02,   1.15090003e-01,  -1.48189999e-02,\n",
       "        -1.65626665e-01,  -5.95399986e-02,  -2.20076666e-04,\n",
       "        -2.18866666e-01,   9.28100049e-02,  -4.92233336e-02,\n",
       "        -1.85590009e-01,   4.88599986e-02,  -3.16983337e-03,\n",
       "         3.88599994e-03,   3.40133334e-02,  -4.26400006e-02,\n",
       "        -2.81433324e-01,  -4.06033322e-02,  -5.60033321e-03,\n",
       "        -1.10929996e-01,  -5.17333349e-02,  -7.71033317e-02,\n",
       "        -6.39366657e-02,  -6.27433340e-01,  -2.55819996e-01,\n",
       "         3.30169996e-02,  -1.40416662e-01,  -6.50866677e-02,\n",
       "         1.33570004e+00,  -6.19799991e-02,  -1.74290001e-01,\n",
       "        -1.05603337e-01,   1.97376668e-04,   2.48163333e-03,\n",
       "         5.92600008e-02,  -5.29899995e-02,   4.01366657e-03,\n",
       "        -1.80743337e-02,  -9.95699962e-02,  -5.24966667e-02,\n",
       "        -1.15859995e-01,  -1.52123335e-02,  -1.47503336e-01,\n",
       "         6.26166662e-02,   9.28299967e-04,  -6.13700002e-02,\n",
       "        -3.83799995e-02,  -2.61936665e-01])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, n_fact, input_length=review_len, weights=[emb], trainable=False), \n",
    "        #vocab_size -word IDs are not used mathematically, they used an index to look up into integer\n",
    "        #n_fact (50) -each word in our vocabulary of 5000 is being converted into a vector of 50 elements\n",
    "        #weights=[emb] -our pretrained embeddings\n",
    "        #training=False -since we think that our pretrained embeddings are pretty good. We start with False, but we can't leave it because of words differencies in words of IMDb and Glove \n",
    "    SpatialDropout1D(0.2),\n",
    "        #dropout applied to the embedding layer zeroes out at random 20% of each of 32 embeddings (i.e. 20% of each word). Its avoiding overfitting the specifics of each word's embeddding    \n",
    "        #a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior\n",
    "    Dropout(0.25), \n",
    "        #dropout after Dense layer is removing at random some of the words effectively (some of the whole vectors)\n",
    "    Convolution1D(64, 5, activation='relu', padding='same'),\n",
    "        #sentences are in 1D convolution, so model would be 1D convolution\n",
    "        #64 - how many filters do you want to create\n",
    "        #5 - size of model convolution\n",
    "        #padding=\"same\" instead of border_mode='same' \n",
    "    Dropout(0.25),\n",
    "        #put same amout of Dropout seems to work well\n",
    "    MaxPooling1D(),\n",
    "        #Conv, Drop, Maxp -simplest CNN \n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 50)           250000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 500, 50)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 64)           16064     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               1600100   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,866,265\n",
      "Trainable params: 1,616,265\n",
      "Non-trainable params: 250,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 788s 32ms/step - loss: 0.6762 - acc: 0.5762 - val_loss: 0.6242 - val_acc: 0.6474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xecfdb7630>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(reviews_train_padded, labels_train, validation_data=(reviews_test_padded, labels_test), epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_sample = \"the movie was terrible you could not find any worse actors that were in the film that's an objective perspective on that kind of garbage product you can have nowadays in our hollywood i was terribly surprised how the plot of this picture is linear actions developing slowly that you want to fall asleep all the time there were no kissing scenes in the whole movie i definitely wouldn't recommend that movie to watch you can watch it if only you are a boring person who has a bunch of a time i guess i will not watch any movie with those actors i saw ever \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_sample_2 = \"the movie was awesome you could not find any better actors that were in the film that's an objective perspective on that kind of glorious product you can have nowadays in our hollywood i was kindly surprised how the plot of this picture is linear actions developing fast that you want to awake all the time there were kissing scenes in the whole movie i definitely would recommend that movie to watch you can watch it if only you are a bright person who has a valuable time i guess i will watch any movie with those actors i saw again \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a specific function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_prediction(x):\n",
    "    assert len(x)>500, 'review length should be less than 500 words'\n",
    "    review_sample_words = x.split( )\n",
    "    review_sample_idx = [dataset_words_idx[word] for word in review_sample_words]\n",
    "    review_sample_idx_trn = np.array([num if num<5000-1 else 5000-1 for num in review_sample_idx])\n",
    "    review_sample_idx_trn_2 = np.array([review_sample_idx_trn])\n",
    "    review_sample_idx_trn_2_padded = pad_sequences(review_sample_idx_trn_2, maxlen=500, value=0)\n",
    "    y = model.predict(review_sample_idx_trn_2_padded, verbose=0)\n",
    "    if y[0] > 0.5:\n",
    "        print(\"Prediction is: \", y, '\\nReview has positive sentiment')\n",
    "    else:\n",
    "        print(\"Prediction is: \", y, '\\nReview has negative sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is:  [[ 0.43317056]] \n",
      "Review has negative sentiment\n"
     ]
    }
   ],
   "source": [
    "review_prediction(review_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is:  [[ 0.56625235]] \n",
      "Review has positive sentiment\n"
     ]
    }
   ],
   "source": [
    "review_prediction(review_sample_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+\"CNN_weights_1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a neural network to reach out state of the art result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].Training=True #we going to fine-tune embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/12\n",
      "25000/25000 [==============================] - 600s 24ms/step - loss: 0.5759 - acc: 0.7120 - val_loss: 0.5067 - val_acc: 0.7906\n",
      "Epoch 2/12\n",
      "25000/25000 [==============================] - 623s 25ms/step - loss: 0.5240 - acc: 0.7469 - val_loss: 0.4632 - val_acc: 0.8046\n",
      "Epoch 3/12\n",
      "25000/25000 [==============================] - 601s 24ms/step - loss: 0.4978 - acc: 0.7659 - val_loss: 0.4343 - val_acc: 0.8182\n",
      "Epoch 4/12\n",
      "25000/25000 [==============================] - 627s 25ms/step - loss: 0.4696 - acc: 0.7834 - val_loss: 0.4195 - val_acc: 0.8240\n",
      "Epoch 5/12\n",
      "25000/25000 [==============================] - 604s 24ms/step - loss: 0.4581 - acc: 0.7894 - val_loss: 0.4296 - val_acc: 0.7969\n",
      "Epoch 6/12\n",
      "25000/25000 [==============================] - 641s 26ms/step - loss: 0.4380 - acc: 0.7962 - val_loss: 0.4026 - val_acc: 0.8191\n",
      "Epoch 7/12\n",
      "25000/25000 [==============================] - 608s 24ms/step - loss: 0.4228 - acc: 0.8078 - val_loss: 0.3870 - val_acc: 0.8290\n",
      "Epoch 8/12\n",
      "25000/25000 [==============================] - 595s 24ms/step - loss: 0.4152 - acc: 0.8103 - val_loss: 0.3938 - val_acc: 0.8210\n",
      "Epoch 9/12\n",
      "25000/25000 [==============================] - 597s 24ms/step - loss: 0.4073 - acc: 0.8172 - val_loss: 0.3826 - val_acc: 0.8326\n",
      "Epoch 10/12\n",
      "25000/25000 [==============================] - 598s 24ms/step - loss: 0.3975 - acc: 0.8240 - val_loss: 0.3862 - val_acc: 0.8239\n",
      "Epoch 11/12\n",
      "25000/25000 [==============================] - 601s 24ms/step - loss: 0.3898 - acc: 0.8267 - val_loss: 0.3999 - val_acc: 0.8144\n",
      "Epoch 12/12\n",
      "25000/25000 [==============================] - 598s 24ms/step - loss: 0.3794 - acc: 0.8294 - val_loss: 0.3794 - val_acc: 0.8291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf0fe4dcc0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(reviews_train_padded, labels_train, validation_data=(reviews_test_padded, labels_test), epochs=12, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/12\n",
      "25000/25000 [==============================] - 590s 24ms/step - loss: 0.3774 - acc: 0.8315 - val_loss: 0.3694 - val_acc: 0.8352\n",
      "Epoch 2/12\n",
      "25000/25000 [==============================] - 590s 24ms/step - loss: 0.3602 - acc: 0.8386 - val_loss: 0.3815 - val_acc: 0.8276\n",
      "Epoch 3/12\n",
      "25000/25000 [==============================] - 588s 24ms/step - loss: 0.3656 - acc: 0.8357 - val_loss: 0.3728 - val_acc: 0.8330\n",
      "Epoch 4/12\n",
      "25000/25000 [==============================] - 582s 23ms/step - loss: 0.3535 - acc: 0.8420 - val_loss: 0.3795 - val_acc: 0.8287\n",
      "Epoch 5/12\n",
      "25000/25000 [==============================] - 581s 23ms/step - loss: 0.3456 - acc: 0.8484 - val_loss: 0.3728 - val_acc: 0.8322\n",
      "Epoch 6/12\n",
      "25000/25000 [==============================] - 581s 23ms/step - loss: 0.3433 - acc: 0.8485 - val_loss: 0.3762 - val_acc: 0.8325\n",
      "Epoch 7/12\n",
      "25000/25000 [==============================] - 581s 23ms/step - loss: 0.3351 - acc: 0.8538 - val_loss: 0.3819 - val_acc: 0.8262\n",
      "Epoch 8/12\n",
      "25000/25000 [==============================] - 583s 23ms/step - loss: 0.3296 - acc: 0.8534 - val_loss: 0.3915 - val_acc: 0.8226\n",
      "Epoch 9/12\n",
      "25000/25000 [==============================] - 582s 23ms/step - loss: 0.3238 - acc: 0.8617 - val_loss: 0.3849 - val_acc: 0.8252\n",
      "Epoch 10/12\n",
      "25000/25000 [==============================] - 585s 23ms/step - loss: 0.3253 - acc: 0.8567 - val_loss: 0.4009 - val_acc: 0.8172\n",
      "Epoch 11/12\n",
      "25000/25000 [==============================] - 627s 25ms/step - loss: 0.3155 - acc: 0.8616 - val_loss: 0.3780 - val_acc: 0.8386\n",
      "Epoch 12/12\n",
      "25000/25000 [==============================] - 591s 24ms/step - loss: 0.3142 - acc: 0.8643 - val_loss: 0.3789 - val_acc: 0.8342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf0fe4d160>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(reviews_train_padded, labels_train, validation_data=(reviews_test_padded, labels_test), epochs=12, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/12\n",
      "25000/25000 [==============================] - 602s 24ms/step - loss: 0.3098 - acc: 0.8668 - val_loss: 0.3848 - val_acc: 0.8299\n",
      "Epoch 2/12\n",
      "25000/25000 [==============================] - 596s 24ms/step - loss: 0.3017 - acc: 0.8693 - val_loss: 0.3957 - val_acc: 0.8220\n",
      "Epoch 3/12\n",
      "25000/25000 [==============================] - 566s 23ms/step - loss: 0.3068 - acc: 0.8690 - val_loss: 0.3759 - val_acc: 0.8346\n",
      "Epoch 4/12\n",
      "25000/25000 [==============================] - 565s 23ms/step - loss: 0.2989 - acc: 0.8725 - val_loss: 0.4123 - val_acc: 0.8137\n",
      "Epoch 5/12\n",
      "25000/25000 [==============================] - 536s 21ms/step - loss: 0.3000 - acc: 0.8713 - val_loss: 0.3756 - val_acc: 0.8370\n",
      "Epoch 6/12\n",
      "25000/25000 [==============================] - 564s 23ms/step - loss: 0.2897 - acc: 0.8738 - val_loss: 0.3842 - val_acc: 0.8318\n",
      "Epoch 7/12\n",
      "25000/25000 [==============================] - 568s 23ms/step - loss: 0.2941 - acc: 0.8742 - val_loss: 0.3921 - val_acc: 0.8324\n",
      "Epoch 8/12\n",
      "25000/25000 [==============================] - 572s 23ms/step - loss: 0.2905 - acc: 0.8754 - val_loss: 0.3862 - val_acc: 0.8284\n",
      "Epoch 9/12\n",
      "25000/25000 [==============================] - 570s 23ms/step - loss: 0.2855 - acc: 0.8777 - val_loss: 0.4158 - val_acc: 0.8182\n",
      "Epoch 10/12\n",
      "25000/25000 [==============================] - 585s 23ms/step - loss: 0.2820 - acc: 0.8807 - val_loss: 0.4328 - val_acc: 0.8123\n",
      "Epoch 11/12\n",
      "25000/25000 [==============================] - 637s 25ms/step - loss: 0.2738 - acc: 0.8806 - val_loss: 0.4069 - val_acc: 0.8210\n",
      "Epoch 12/12\n",
      "25000/25000 [==============================] - 583s 23ms/step - loss: 0.2731 - acc: 0.8828 - val_loss: 0.3985 - val_acc: 0.8251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf0fe4d390>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(reviews_train_padded, labels_train, validation_data=(reviews_test_padded, labels_test), epochs=12, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/12\n",
      "25000/25000 [==============================] - 590s 24ms/step - loss: 0.2724 - acc: 0.8819 - val_loss: 0.4038 - val_acc: 0.8228\n",
      "Epoch 2/12\n",
      "25000/25000 [==============================] - 578s 23ms/step - loss: 0.2736 - acc: 0.8816 - val_loss: 0.4116 - val_acc: 0.8198\n",
      "Epoch 3/12\n",
      "25000/25000 [==============================] - 578s 23ms/step - loss: 0.2712 - acc: 0.8841 - val_loss: 0.4007 - val_acc: 0.8239\n",
      "Epoch 4/12\n",
      "25000/25000 [==============================] - 576s 23ms/step - loss: 0.2653 - acc: 0.8887 - val_loss: 0.3976 - val_acc: 0.8266\n",
      "Epoch 5/12\n",
      "25000/25000 [==============================] - 587s 23ms/step - loss: 0.2622 - acc: 0.8886 - val_loss: 0.3963 - val_acc: 0.8306\n",
      "Epoch 6/12\n",
      "25000/25000 [==============================] - 648s 26ms/step - loss: 0.2621 - acc: 0.8894 - val_loss: 0.4013 - val_acc: 0.8232\n",
      "Epoch 7/12\n",
      "25000/25000 [==============================] - 583s 23ms/step - loss: 0.2576 - acc: 0.8910 - val_loss: 0.4228 - val_acc: 0.8161\n",
      "Epoch 8/12\n",
      "25000/25000 [==============================] - 638s 26ms/step - loss: 0.2596 - acc: 0.8911 - val_loss: 0.4111 - val_acc: 0.8250\n",
      "Epoch 9/12\n",
      "25000/25000 [==============================] - 591s 24ms/step - loss: 0.2582 - acc: 0.8901 - val_loss: 0.4441 - val_acc: 0.8062\n",
      "Epoch 10/12\n",
      "25000/25000 [==============================] - 579s 23ms/step - loss: 0.2551 - acc: 0.8926 - val_loss: 0.3987 - val_acc: 0.8258\n",
      "Epoch 11/12\n",
      "25000/25000 [==============================] - 580s 23ms/step - loss: 0.2561 - acc: 0.8932 - val_loss: 0.4038 - val_acc: 0.8284\n",
      "Epoch 12/12\n",
      "25000/25000 [==============================] - 587s 23ms/step - loss: 0.2501 - acc: 0.8937 - val_loss: 0.4158 - val_acc: 0.8236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf0fe4d240>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(reviews_train_padded, labels_train, validation_data=(reviews_test_padded, labels_test), epochs=12, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+\"CNN_weights_1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is:  [[ 0.01205496]] \n",
      "Review has negative sentiment\n"
     ]
    }
   ],
   "source": [
    "review_prediction(review_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is:  [[ 0.92599201]] \n",
      "Review has positive sentiment\n"
     ]
    }
   ],
   "source": [
    "review_prediction(review_sample_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
