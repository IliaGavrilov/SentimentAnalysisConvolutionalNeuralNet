{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis CNN in Keras - example of using IMDb dataset with Glove pretrained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division, print_function \n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Activation, Dropout, Flatten, Convolution1D, SpatialDropout1D, MaxPooling1D\n",
    "from keras import metrics #Only one metric is supported at the moment and that is accuracy\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'C:/Users/Gavrilov/My Projects/Sentiment Analysis CNN/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb \n",
    "    #dataset of 25,000 movies reviews from IMDB #https://keras.io/datasets/\n",
    "    #each review labeled by sentiment (positive/negative)\n",
    "    #each review is encoded as a sequence of word indexes (integers)\n",
    "    #words indexed by overall frequency in the dataset (integer \"3\" encodes the 3rd most frequent word)\n",
    "    #\"0\" does not stand for a specific word, but instead is used to encode any unknown word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Keras words frequency - dictionary manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dictionary of word (key) and index (value) - which is necessary when passing indices to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "dataset_words_idx = imdb.get_word_index() #IMDb indices assignment to idx variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of word 'the' is 1\n",
      "{'fawn': 34701, 'tsukino': 52006, 'nunnery': 52007, 'sonja': 16816, 'vani': 63951, 'woods': 1408, 'spiders': 16115, 'hanging': 2345, 'woody': 2289, 'trawling': 52008, \"hold's\": 52009, 'comically': 11307, 'localized': 40830, 'disobeying': 30568, \"'royale\": 52010, \"harpo's\": 40831, 'canet': 52011, 'aileen': 19313, 'acurately': 52012, \"diplomat's\": 52013}\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of word 'the' is\", dataset_words_idx['the'])\n",
    "print({i:dataset_words_idx[i] for i in list(dataset_words_idx)[:20]}) #print out some elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dictionary of index (value) and word (key) - which is necessary when decoding models predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_idx_words = {value: key for key, value in iter(dataset_words_idx.items())} \n",
    "    #mapping of words to indices will give dictionary, where key and value positions switched\n",
    "#dataset_idx_words = {}\n",
    "#for key, value in iter(dataset_words_idx.items()): #iter(ind_words.items()) -iterates over the dictionary returning key, value \n",
    "#    dataset_idx_words[value]=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fawn tsukino nunnery\n",
      "{34701: 'fawn', 52006: 'tsukino', 52007: 'nunnery', 16816: 'sonja', 63951: 'vani', 1408: 'woods', 16115: 'spiders', 2345: 'hanging', 2289: 'woody', 52008: 'trawling', 52009: \"hold's\", 11307: 'comically', 40830: 'localized', 30568: 'disobeying', 52010: \"'royale\", 40831: \"harpo's\", 52011: 'canet', 19313: 'aileen', 52012: 'acurately', 52013: \"diplomat's\"}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_idx_words[34701], dataset_idx_words[52006], dataset_idx_words[52007])\n",
    "print({i:dataset_idx_words[i] for i in list(dataset_idx_words)[:20]}) #printing out of mapped elements of new dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset frequent words in sorted array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fawn tsukino nunnery\n",
      "Most frequent words are:  ['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i', 'this', 'that', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'on']\n"
     ]
    }
   ],
   "source": [
    "dataset_words_arr_sorted = sorted(dataset_words_idx, key=dataset_words_idx.get)\n",
    "    #sorted() returns sorted list but not mutates the original list\n",
    "    #idx is iterable (words in our example), key function will sort the given iterable\n",
    "    #idx.get returns value None because value in idx.get() not specified\n",
    "print(dataset_words_arr_sorted[34700], dataset_words_arr_sorted[52005], dataset_words_arr_sorted[52006])\n",
    "    #words from dataset dictionary by specific index\n",
    "print('Most frequent words are: ', dataset_words_arr_sorted[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using IMDB Keras dataset directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size=5000 #truncate vocabulary down to 5000\n",
    "review_len=500 #truncate every review to 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 12s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(reviews_train, labels_train), (reviews_test, labels_test) = imdb.load_data(path='imdb.npz',\n",
    "                                                      num_words=vocab_size, #top most frequent words \n",
    "                                                      #skip_top=0, #top most frequent words to ignore\n",
    "                                                      #maxlen=review_len, \n",
    "                                                      #any longer sequence will be truncated\n",
    "                                                      #will not use it here because it will cause \n",
    "                                                      #disproportion in shapes reviews_train, reviews_test\n",
    "                                                      #will truncate manually using Keras sequence padding\n",
    "                                                      #seed=500, \n",
    "                                                      start_char=1, \n",
    "                                                      #start of a sequence will be marked with this char\n",
    "                                                      #set to 1 because 0 is usually the padding char                                                      #ovv_char=0, #words that were cut out \n",
    "                                                      #because of the num_words or skip_top limit will be \n",
    "                                                      #replaced with this character  \n",
    "                                                      index_from=0) \n",
    "                                                      #index actual words with this index and higher\n",
    "#reviews_train, reviews_test -list of sequences, which are lists of indexes (integers) (list or reviews which captures list of indices of words)\n",
    "#labels_train, labels_test -list of integer labels (1 positive or 0 negative sentiment) given to reviews in dataset\n",
    "#labeled reviews it's pretty much same as labeled pictures in Dogs vs Cats task\n",
    "#what the convolutional neural net does is learning to predict where is negarive or positive review on unlabeled data (Dog vs Cat in the example with pictures) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_train), len(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert and is an amazing actor and now the same being director and father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for and and would recommend it to everyone to watch and the fly and was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also and to the two little and that played the and of norman and paul they were just brilliant children are often left out of the and list i think because the stars that play them all grown up are such a big and for the whole film but these children are amazing and should be and for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was and with us all\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([dataset_idx_words[i] for i in reviews_train[0]]) \n",
    "#so our goal is to take 25,000 reviews, and predict whether it will be positive or negative in sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0], dtype=int64),\n",
       " array([0, 1, 1, 0, 1, 1, 1, 0, 0, 1], dtype=int64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[:10], labels_test[:10]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rectangular matrix - Zero padding of each sequence for making consistent length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train.shape, labels_test.shape #given shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141, 603)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_train[2]), len(reviews_test[2]) #we need to make all of reviews the same lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_train_padded = pad_sequences(reviews_train, maxlen=500, value=0) \n",
    "reviews_test_padded = pad_sequences(reviews_test, maxlen=500, value=0)\n",
    "#truncates everything greater than maxlen, but padds everything with given value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 500), (25000, 500))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train_padded.shape, reviews_test_padded.shape #at the end of this we have numpy array with identical shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train_padded.shape==reviews_test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_train_padded[2])==len(reviews_test_padded[2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Pretrained Word Embeddings setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://nlp.stanford.edu/data/glove.6B.zip\n",
      "862183424/862182613 [==============================] - 448s 1us/step\n"
     ]
    }
   ],
   "source": [
    "glove_6B = get_file('glove.6B.zip', origin='http://nlp.stanford.edu/data/glove.6B.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unzipping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(glove_6B, 'r') as zip_ref:\n",
    "    zip_ref.extractall('C:/Users/Gavrilov/.keras/datasets/glove_6B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### setting up a variable of 6B version tokens Glove with 50 dimensions words embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = \"glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reading file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('C:/Users/Gavrilov/.keras/datasets/glove_6B/'+name, 'r', encoding=\"utf8\") as f: \n",
    "    lines = [line.split() for line in f] #lines.split() -without value '\\n' because each iteration gives line and that is fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### setting up main arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_words = [elem[0] for elem in lines]\n",
    "glove_words_idx = {elem:idx for idx,elem in enumerate(glove_words)} #is elem:idx equal to glove_words_idx[elem]=idx?\n",
    "glove_vecs = np.stack(np.array(elem[1:], dtype=np.float32) for elem in lines) #np.float32 -standard double-precision floating point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the , . of to and in a \" \\'s for - that on is was said with he as',\n",
       " 0,\n",
       " (400000, 50),\n",
       " array([  4.18000013e-01,   2.49679998e-01,  -4.12420005e-01,\n",
       "          1.21699996e-01,   3.45270008e-01,  -4.44569997e-02,\n",
       "         -4.96879995e-01,  -1.78619996e-01,  -6.60229998e-04,\n",
       "         -6.56599998e-01,   2.78430015e-01,  -1.47670001e-01,\n",
       "         -5.56770027e-01,   1.46579996e-01,  -9.50950012e-03,\n",
       "          1.16579998e-02,   1.02040000e-01,  -1.27920002e-01,\n",
       "         -8.44299972e-01,  -1.21809997e-01,  -1.68009996e-02,\n",
       "         -3.32789987e-01,  -1.55200005e-01,  -2.31309995e-01,\n",
       "         -1.91809997e-01,  -1.88230002e+00,  -7.67459989e-01,\n",
       "          9.90509987e-02,  -4.21249986e-01,  -1.95260003e-01,\n",
       "          4.00710011e+00,  -1.85939997e-01,  -5.22870004e-01,\n",
       "         -3.16810012e-01,   5.92130003e-04,   7.44489999e-03,\n",
       "          1.77780002e-01,  -1.58969998e-01,   1.20409997e-02,\n",
       "         -5.42230010e-02,  -2.98709989e-01,  -1.57490000e-01,\n",
       "         -3.47579986e-01,  -4.56370004e-02,  -4.42510009e-01,\n",
       "          1.87849998e-01,   2.78489990e-03,  -1.84110001e-01,\n",
       "         -1.15139998e-01,  -7.85809994e-01], dtype=float32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(glove_words[:20]), glove_words_idx['the'], glove_vecs.shape, glove_vecs[glove_words_idx['the']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(glove_words, open('C:/Users/Gavrilov/.keras/datasets/glove_6B/'+name+'_glove_words.pkl', 'wb'))\n",
    "pickle.dump(glove_words_idx, open('C:/Users/Gavrilov/.keras/datasets/glove_6B/'+name+'_glove_words_idx.pkl', 'wb'))\n",
    "np.save('C:/Users/Gavrilov/.keras/datasets/glove_6B/'+name+'_glove_vecs', glove_vecs, allow_pickle=True) #file would have a name glove.6B.50d.txt.dat which is a little buggy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding matrix - extracting indices and vectors from Glove regarding words in IMDb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fact = glove_vecs.shape[1] #getting value, which is same as dimensions of Glove vectors (amount of floats in words array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = np.zeros((vocab_size, n_fact)) #create an array of zeros \n",
    "#creating a numpy array 2D tempalate with number of axes values same as our vocab_size () and dimentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 50),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape, emb[1] #is shape of our future array shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400000, 50),\n",
       " array([ 0.013441  ,  0.23682   , -0.16899   ,  0.40950999,  0.63812   ,\n",
       "         0.47709   , -0.42851999, -0.55641001, -0.36399999, -0.23938   ,\n",
       "         0.13000999, -0.063734  , -0.39574999, -0.48162001,  0.23291001,\n",
       "         0.090201  , -0.13324   ,  0.078639  , -0.41633999, -0.15428001,\n",
       "         0.10068   ,  0.48890999,  0.31226   , -0.1252    , -0.037512  ,\n",
       "        -1.51789999,  0.12612   , -0.02442   , -0.042961  , -0.28351   ,\n",
       "         3.54159999, -0.11956   , -0.014533  , -0.1499    ,  0.21864   ,\n",
       "        -0.33412001, -0.13872001,  0.31806001,  0.70358002,  0.44858   ,\n",
       "        -0.080262  ,  0.63002998,  0.32111001, -0.46765   ,  0.22786   ,\n",
       "         0.36034   , -0.37818   , -0.56656998,  0.044691  ,  0.30392   ], dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vecs.shape, glove_vecs[1] #is shape of exist Glove numpy array shape, from wich we will chose our vectors for words in our vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num in range(1, len(emb)): #mapping words vectors from Glove to IMDb words\n",
    "    word = dataset_idx_words[num]\n",
    "    if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word): \n",
    "        #bool to find if word match with all variances it could appear (which can happen in Glove) \n",
    "        #using regular expression (see http://www.regular-expressions.info/python.html)\n",
    "        temp_idx = glove_words_idx[word] #getting index of word in Glove dictionary\n",
    "        emb[num] = glove_vecs[temp_idx] \n",
    "        #assigning of created emb matrix of given temp_idx dimensions \n",
    "        #with relevant values from Glove word embeddings \n",
    "    else: #if no match identified (if we can't find a match in Glove, will just assign to random value)\n",
    "        emb[num] = np.random.normal(scale=0.6, size=(n_fact,)) \n",
    "        #have had to create random embedding, because sometimes the word we've lokked in IMDb \n",
    "        #didn't exist in Glove (for example, words with apostrophe 's')\n",
    "        #basically whole preprocess with Glove is only needed for extracting vectors of words, \n",
    "        #wich then becomes part of matrix embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.18000013e-01,   2.49679998e-01,  -4.12420005e-01,\n",
       "         1.21699996e-01,   3.45270008e-01,  -4.44569997e-02,\n",
       "        -4.96879995e-01,  -1.78619996e-01,  -6.60229998e-04,\n",
       "        -6.56599998e-01,   2.78430015e-01,  -1.47670001e-01,\n",
       "        -5.56770027e-01,   1.46579996e-01,  -9.50950012e-03,\n",
       "         1.16579998e-02,   1.02040000e-01,  -1.27920002e-01,\n",
       "        -8.44299972e-01,  -1.21809997e-01,  -1.68009996e-02,\n",
       "        -3.32789987e-01,  -1.55200005e-01,  -2.31309995e-01,\n",
       "        -1.91809997e-01,  -1.88230002e+00,  -7.67459989e-01,\n",
       "         9.90509987e-02,  -4.21249986e-01,  -1.95260003e-01,\n",
       "         4.00710011e+00,  -1.85939997e-01,  -5.22870004e-01,\n",
       "        -3.16810012e-01,   5.92130003e-04,   7.44489999e-03,\n",
       "         1.77780002e-01,  -1.58969998e-01,   1.20409997e-02,\n",
       "        -5.42230010e-02,  -2.98709989e-01,  -1.57490000e-01,\n",
       "        -3.47579986e-01,  -4.56370004e-02,  -4.42510009e-01,\n",
       "         1.87849998e-01,   2.78489990e-03,  -1.84110001e-01,\n",
       "        -1.15139998e-01,  -7.85809994e-01])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb[-1] = np.random.normal(scale=0.6, size=(n_fact,)) #this is our \"rare word\" id - we want to randomly initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb/=3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.39333338e-01,   8.32266659e-02,  -1.37473335e-01,\n",
       "         4.05666654e-02,   1.15090003e-01,  -1.48189999e-02,\n",
       "        -1.65626665e-01,  -5.95399986e-02,  -2.20076666e-04,\n",
       "        -2.18866666e-01,   9.28100049e-02,  -4.92233336e-02,\n",
       "        -1.85590009e-01,   4.88599986e-02,  -3.16983337e-03,\n",
       "         3.88599994e-03,   3.40133334e-02,  -4.26400006e-02,\n",
       "        -2.81433324e-01,  -4.06033322e-02,  -5.60033321e-03,\n",
       "        -1.10929996e-01,  -5.17333349e-02,  -7.71033317e-02,\n",
       "        -6.39366657e-02,  -6.27433340e-01,  -2.55819996e-01,\n",
       "         3.30169996e-02,  -1.40416662e-01,  -6.50866677e-02,\n",
       "         1.33570004e+00,  -6.19799991e-02,  -1.74290001e-01,\n",
       "        -1.05603337e-01,   1.97376668e-04,   2.48163333e-03,\n",
       "         5.92600008e-02,  -5.29899995e-02,   4.01366657e-03,\n",
       "        -1.80743337e-02,  -9.95699962e-02,  -5.24966667e-02,\n",
       "        -1.15859995e-01,  -1.52123335e-02,  -1.47503336e-01,\n",
       "         6.26166662e-02,   9.28299967e-04,  -6.13700002e-02,\n",
       "        -3.83799995e-02,  -2.61936665e-01])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, n_fact, input_length=review_len, weights=[emb], trainable=False), \n",
    "        #vocab_size -word IDs are not used mathematically, they used an index to look up into integer\n",
    "        #n_fact (50) -each word in our vocabulary of 5000 is being converted into a vector of 50 elements\n",
    "        #weights=[emb] -our pretrained embeddings\n",
    "        #training=False -since we think that our pretrained embeddings are pretty good. \n",
    "        #We start with False, but we can't leave it because of differencies in words of IMDb and Glove \n",
    "    SpatialDropout1D(0.2),\n",
    "        #dropout applied to the embedding layer zeroes out at random 20% of each of 32 embeddings \n",
    "        #(i.e. 20% of each word). \n",
    "        #Its avoiding overfitting the specifics of each word's embeddding    \n",
    "        #a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get same behavior\n",
    "    Dropout(0.25), \n",
    "        #dropout after Dense layer is removing at random some of the words effectively \n",
    "        #(some of the whole vectors)\n",
    "    Convolution1D(64, 5, activation='relu', padding='same'),\n",
    "        #sentences are in 1D convolution, so model would be 1D convolution\n",
    "        #64 - how many filters do you want to create\n",
    "        #5 - size of model convolution\n",
    "        #padding=\"same\" instead of border_mode='same' \n",
    "    Dropout(0.25),\n",
    "        #put same amout of Dropout seems to work well\n",
    "    MaxPooling1D(),\n",
    "        #Conv, Drop, Maxp -simplest CNN \n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 50)           250000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 500, 50)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 64)           16064     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               1600100   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,866,265\n",
      "Trainable params: 1,616,265\n",
      "Non-trainable params: 250,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 788s 32ms/step - loss: 0.6762 - acc: 0.5762 - val_loss: 0.6242 - val_acc: 0.6474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xecfdb7630>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(reviews_train_padded, labels_train, validation_data=(reviews_test_padded, labels_test), epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_sample = \"the movie was terrible you could not find any worse actors that were in the film that's an objective perspective on that kind of garbage product you can have nowadays in our hollywood i was terribly surprised how the plot of this picture is linear actions developing slowly that you want to fall asleep all the time there were no kissing scenes in the whole movie i definitely wouldn't recommend that movie to watch you can watch it if only you are a boring person who has a bunch of a time i guess i will not watch any movie with those actors i saw ever \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_sample_2 = \"the movie was awesome you could not find any better actors that were in the film that's an objective perspective on that kind of glorious product you can have nowadays in our hollywood i was kindly surprised how the plot of this picture is linear actions developing fast that you want to awake all the time there were kissing scenes in the whole movie i definitely would recommend that movie to watch you can watch it if only you are a bright person who has a valuable time i guess i will watch any movie with those actors i saw again \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a specific function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_prediction(x):\n",
    "    assert len(x)>500, 'review length should be less than 500 words'\n",
    "    review_sample_words = x.split( )\n",
    "    review_sample_idx = [dataset_words_idx[word] for word in review_sample_words]\n",
    "    review_sample_idx_trn = np.array([num if num<5000-1 else 5000-1 for num in review_sample_idx])\n",
    "    review_sample_idx_trn_2 = np.array([review_sample_idx_trn])\n",
    "    review_sample_idx_trn_2_padded = pad_sequences(review_sample_idx_trn_2, maxlen=500, value=0)\n",
    "    y = model.predict(review_sample_idx_trn_2_padded, verbose=0)\n",
    "    if y[0] > 0.5:\n",
    "        print(\"Prediction is: \", y, '\\nReview has positive sentiment')\n",
    "    else:\n",
    "        print(\"Prediction is: \", y, '\\nReview has negative sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is:  [[ 0.43317056]] \n",
      "Review has negative sentiment\n"
     ]
    }
   ],
   "source": [
    "review_prediction(review_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is:  [[ 0.56625235]] \n",
      "Review has positive sentiment\n"
     ]
    }
   ],
   "source": [
    "review_prediction(review_sample_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+\"CNN_weights_1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a neural network to reach out state of the art result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].Training=True #we going to fine-tune embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/12\n",
      "25000/25000 [==============================] - 600s 24ms/step - loss: 0.5759 - acc: 0.7120 - val_loss: 0.5067 - val_acc: 0.7906\n",
      "Epoch 2/12\n",
      "25000/25000 [==============================] - 623s 25ms/step - loss: 0.5240 - acc: 0.7469 - val_loss: 0.4632 - val_acc: 0.8046\n",
      "Epoch 3/12\n",
      "25000/25000 [==============================] - 601s 24ms/step - loss: 0.4978 - acc: 0.7659 - val_loss: 0.4343 - val_acc: 0.8182\n",
      "Epoch 4/12\n",
      "25000/25000 [==============================] - 627s 25ms/step - loss: 0.4696 - acc: 0.7834 - val_loss: 0.4195 - val_acc: 0.8240\n",
      "Epoch 5/12\n",
      "25000/25000 [==============================] - 604s 24ms/step - loss: 0.4581 - acc: 0.7894 - val_loss: 0.4296 - val_acc: 0.7969\n",
      "Epoch 6/12\n",
      "25000/25000 [==============================] - 641s 26ms/step - loss: 0.4380 - acc: 0.7962 - val_loss: 0.4026 - val_acc: 0.8191\n",
      "Epoch 7/12\n",
      "25000/25000 [==============================] - 608s 24ms/step - loss: 0.4228 - acc: 0.8078 - val_loss: 0.3870 - val_acc: 0.8290\n",
      "Epoch 8/12\n",
      "25000/25000 [==============================] - 595s 24ms/step - loss: 0.4152 - acc: 0.8103 - val_loss: 0.3938 - val_acc: 0.8210\n",
      "Epoch 9/12\n",
      "25000/25000 [==============================] - 597s 24ms/step - loss: 0.4073 - acc: 0.8172 - val_loss: 0.3826 - val_acc: 0.8326\n",
      "Epoch 10/12\n",
      "25000/25000 [==============================] - 598s 24ms/step - loss: 0.3975 - acc: 0.8240 - val_loss: 0.3862 - val_acc: 0.8239\n",
      "Epoch 11/12\n",
      "25000/25000 [==============================] - 601s 24ms/step - loss: 0.3898 - acc: 0.8267 - val_loss: 0.3999 - val_acc: 0.8144\n",
      "Epoch 12/12\n",
      "25000/25000 [==============================] - 598s 24ms/step - loss: 0.3794 - acc: 0.8294 - val_loss: 0.3794 - val_acc: 0.8291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf0fe4dcc0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(reviews_train_padded, labels_train, validation_data=(reviews_test_padded, labels_test), epochs=12, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/12\n",
      "25000/25000 [==============================] - 590s 24ms/step - loss: 0.3774 - acc: 0.8315 - val_loss: 0.3694 - val_acc: 0.8352\n",
      "Epoch 2/12\n",
      "25000/25000 [==============================] - 590s 24ms/step - loss: 0.3602 - acc: 0.8386 - val_loss: 0.3815 - val_acc: 0.8276\n",
      "Epoch 3/12\n",
      "25000/25000 [==============================] - 588s 24ms/step - loss: 0.3656 - acc: 0.8357 - val_loss: 0.3728 - val_acc: 0.8330\n",
      "Epoch 4/12\n",
      "25000/25000 [==============================] - 582s 23ms/step - loss: 0.3535 - acc: 0.8420 - val_loss: 0.3795 - val_acc: 0.8287\n",
      "Epoch 5/12\n",
      "25000/25000 [==============================] - 581s 23ms/step - loss: 0.3456 - acc: 0.8484 - val_loss: 0.3728 - val_acc: 0.8322\n",
      "Epoch 6/12\n",
      "25000/25000 [==============================] - 581s 23ms/step - loss: 0.3433 - acc: 0.8485 - val_loss: 0.3762 - val_acc: 0.8325\n",
      "Epoch 7/12\n",
      "25000/25000 [==============================] - 581s 23ms/step - loss: 0.3351 - acc: 0.8538 - val_loss: 0.3819 - val_acc: 0.8262\n",
      "Epoch 8/12\n",
      "25000/25000 [==============================] - 583s 23ms/step - loss: 0.3296 - acc: 0.8534 - val_loss: 0.3915 - val_acc: 0.8226\n",
      "Epoch 9/12\n",
      "25000/25000 [==============================] - 582s 23ms/step - loss: 0.3238 - acc: 0.8617 - val_loss: 0.3849 - val_acc: 0.8252\n",
      "Epoch 10/12\n",
      "25000/25000 [==============================] - 585s 23ms/step - loss: 0.3253 - acc: 0.8567 - val_loss: 0.4009 - val_acc: 0.8172\n",
      "Epoch 11/12\n",
      "25000/25000 [==============================] - 627s 25ms/step - loss: 0.3155 - acc: 0.8616 - val_loss: 0.3780 - val_acc: 0.8386\n",
      "Epoch 12/12\n",
      "25000/25000 [==============================] - 591s 24ms/step - loss: 0.3142 - acc: 0.8643 - val_loss: 0.3789 - val_acc: 0.8342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf0fe4d160>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(reviews_train_padded, labels_train, validation_data=(reviews_test_padded, labels_test), epochs=12, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/12\n",
      "25000/25000 [==============================] - 602s 24ms/step - loss: 0.3098 - acc: 0.8668 - val_loss: 0.3848 - val_acc: 0.8299\n",
      "Epoch 2/12\n",
      "25000/25000 [==============================] - 596s 24ms/step - loss: 0.3017 - acc: 0.8693 - val_loss: 0.3957 - val_acc: 0.8220\n",
      "Epoch 3/12\n",
      "25000/25000 [==============================] - 566s 23ms/step - loss: 0.3068 - acc: 0.8690 - val_loss: 0.3759 - val_acc: 0.8346\n",
      "Epoch 4/12\n",
      "25000/25000 [==============================] - 565s 23ms/step - loss: 0.2989 - acc: 0.8725 - val_loss: 0.4123 - val_acc: 0.8137\n",
      "Epoch 5/12\n",
      "25000/25000 [==============================] - 536s 21ms/step - loss: 0.3000 - acc: 0.8713 - val_loss: 0.3756 - val_acc: 0.8370\n",
      "Epoch 6/12\n",
      "25000/25000 [==============================] - 564s 23ms/step - loss: 0.2897 - acc: 0.8738 - val_loss: 0.3842 - val_acc: 0.8318\n",
      "Epoch 7/12\n",
      "25000/25000 [==============================] - 568s 23ms/step - loss: 0.2941 - acc: 0.8742 - val_loss: 0.3921 - val_acc: 0.8324\n",
      "Epoch 8/12\n",
      "25000/25000 [==============================] - 572s 23ms/step - loss: 0.2905 - acc: 0.8754 - val_loss: 0.3862 - val_acc: 0.8284\n",
      "Epoch 9/12\n",
      "25000/25000 [==============================] - 570s 23ms/step - loss: 0.2855 - acc: 0.8777 - val_loss: 0.4158 - val_acc: 0.8182\n",
      "Epoch 10/12\n",
      "25000/25000 [==============================] - 585s 23ms/step - loss: 0.2820 - acc: 0.8807 - val_loss: 0.4328 - val_acc: 0.8123\n",
      "Epoch 11/12\n",
      "25000/25000 [==============================] - 637s 25ms/step - loss: 0.2738 - acc: 0.8806 - val_loss: 0.4069 - val_acc: 0.8210\n",
      "Epoch 12/12\n",
      "25000/25000 [==============================] - 583s 23ms/step - loss: 0.2731 - acc: 0.8828 - val_loss: 0.3985 - val_acc: 0.8251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf0fe4d390>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(reviews_train_padded, labels_train, validation_data=(reviews_test_padded, labels_test), epochs=12, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/12\n",
      "25000/25000 [==============================] - 590s 24ms/step - loss: 0.2724 - acc: 0.8819 - val_loss: 0.4038 - val_acc: 0.8228\n",
      "Epoch 2/12\n",
      "25000/25000 [==============================] - 578s 23ms/step - loss: 0.2736 - acc: 0.8816 - val_loss: 0.4116 - val_acc: 0.8198\n",
      "Epoch 3/12\n",
      "25000/25000 [==============================] - 578s 23ms/step - loss: 0.2712 - acc: 0.8841 - val_loss: 0.4007 - val_acc: 0.8239\n",
      "Epoch 4/12\n",
      "25000/25000 [==============================] - 576s 23ms/step - loss: 0.2653 - acc: 0.8887 - val_loss: 0.3976 - val_acc: 0.8266\n",
      "Epoch 5/12\n",
      "25000/25000 [==============================] - 587s 23ms/step - loss: 0.2622 - acc: 0.8886 - val_loss: 0.3963 - val_acc: 0.8306\n",
      "Epoch 6/12\n",
      "25000/25000 [==============================] - 648s 26ms/step - loss: 0.2621 - acc: 0.8894 - val_loss: 0.4013 - val_acc: 0.8232\n",
      "Epoch 7/12\n",
      "25000/25000 [==============================] - 583s 23ms/step - loss: 0.2576 - acc: 0.8910 - val_loss: 0.4228 - val_acc: 0.8161\n",
      "Epoch 8/12\n",
      "25000/25000 [==============================] - 638s 26ms/step - loss: 0.2596 - acc: 0.8911 - val_loss: 0.4111 - val_acc: 0.8250\n",
      "Epoch 9/12\n",
      "25000/25000 [==============================] - 591s 24ms/step - loss: 0.2582 - acc: 0.8901 - val_loss: 0.4441 - val_acc: 0.8062\n",
      "Epoch 10/12\n",
      "25000/25000 [==============================] - 579s 23ms/step - loss: 0.2551 - acc: 0.8926 - val_loss: 0.3987 - val_acc: 0.8258\n",
      "Epoch 11/12\n",
      "25000/25000 [==============================] - 580s 23ms/step - loss: 0.2561 - acc: 0.8932 - val_loss: 0.4038 - val_acc: 0.8284\n",
      "Epoch 12/12\n",
      "25000/25000 [==============================] - 587s 23ms/step - loss: 0.2501 - acc: 0.8937 - val_loss: 0.4158 - val_acc: 0.8236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf0fe4d240>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(reviews_train_padded, labels_train, validation_data=(reviews_test_padded, labels_test), epochs=12, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+\"CNN_weights_1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is:  [[ 0.01205496]] \n",
      "Review has negative sentiment\n"
     ]
    }
   ],
   "source": [
    "review_prediction(review_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is:  [[ 0.92599201]] \n",
      "Review has positive sentiment\n"
     ]
    }
   ],
   "source": [
    "review_prediction(review_sample_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
